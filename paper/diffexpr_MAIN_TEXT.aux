\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\newlabel{FirstPage}{{}{1}{}{section*.1}{}}
\@writefile{toc}{\contentsline {title}{Inferring repertoire dynamics from repertoire sequencing}{1}{section*.2}}
\@writefile{toc}{\contentsline {abstract}{Abstract}{1}{section*.1}}
\@writefile{toc}{\contentsline {section}{\numberline {}Results}{1}{section*.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}A repertoire model family learnable using RepSeq}{1}{section*.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Model family definition}{1}{section*.5}}
\citation{Robinson2008}
\newlabel{eq:jointf}{{1}{2}{}{equation.0.1}{}}
\newlabel{eq:norm_constr}{{2}{2}{}{equation.0.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Normalization}{2}{section*.6}}
\newlabel{eq:orignorm}{{4}{2}{}{equation.0.4}{}}
\newlabel{eq:postnorm}{{5}{2}{}{equation.0.5}{}}
\newlabel{eq:fprimeconst}{{6}{2}{}{equation.0.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Model Assumptions}{2}{section*.7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Learning procedure}{2}{section*.8}}
\citation{Robinson2008}
\citation{Best2015a}
\@writefile{toc}{\contentsline {subsection}{\numberline {}A null model for replicate clone size variation}{3}{section*.9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Model description}{3}{section*.10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Selecting an unbiased normalization constraint}{3}{section*.11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Refinements to the measurement model}{3}{section*.12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Measurement model parameters}{3}{section*.13}}
\citation{Mora2016}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  \emph  {Repertoire model}. (a) Clone frequency distribution, $\rho (f)$, is set as a power law, parameterized by the power, $\alpha $ and the minimum frequency, $f_{min}$. (b) (c) Differential expression model structure and learning procedure. Null model parameters are learned by marginalizing over clone frequency, $f$, and maximizing this marginal likelihood with respect to the parameters. Sampled repertoires can then be generated using the ML estimate. $P(n|f)$ is set as a negative binomial distribution in cell counts controlling the scale parameter of a Poisson distribution of molecule counts. In the differentially expressed condition (prime-decorated quantities), the parameters, $\theta _{\textrm  {diff}}$, of the prior distribution, $\rho (c)$, of fold-change, $c$, are learned by maximizing the marginal likelihood (see section \textit  {Fold change prior}) with respect to $\theta _{\textrm  {diff}}$, keeping the remaining parameters, $\theta _{\textrm  {null}}$, fixed to their ML estimates, $\mathaccentV {hat}05E{\theta }_{\textrm  {null}}$, previously obtained using same-day replicate data (see fig. \ref  {fig:nullstats}). }}{4}{figure.1}}
\newlabel{fig:fullmodel}{{1}{4}{\emph {Repertoire model}. (a) Clone frequency distribution, $\rho (f)$, is set as a power law, parameterized by the power, $\alpha $ and the minimum frequency, $f_{min}$. (b) (c) Differential expression model structure and learning procedure. Null model parameters are learned by marginalizing over clone frequency, $f$, and maximizing this marginal likelihood with respect to the parameters. Sampled repertoires can then be generated using the ML estimate. $P(n|f)$ is set as a negative binomial distribution in cell counts controlling the scale parameter of a Poisson distribution of molecule counts. In the differentially expressed condition (prime-decorated quantities), the parameters, $\theta _{\textrm {diff}}$, of the prior distribution, $\rho (c)$, of fold-change, $c$, are learned by maximizing the marginal likelihood (see section \textit {Fold change prior}) with respect to $\theta _{\textrm {diff}}$, keeping the remaining parameters, $\theta _{\textrm {null}}$, fixed to their ML estimates, $\hat {\theta }_{\textrm {null}}$, previously obtained using same-day replicate data (see fig. \ref {fig:nullstats})}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  \emph  {Learning a null model}. Models and data comparison using molecule pair count statistics (example donor: S2; day 0-day 0 comparison). The learned model for Poisson distributed $P(n|f)$ (left) and for $P(n|f)$ set as a negative binomial distribution in cell counts controlling the scale parameter of a Poisson distribution of molecule counts (center; fig.\ref  {fig:modelstruct}). Right: empirical pair count histogram. }}{4}{figure.2}}
\newlabel{fig:nullstats}{{2}{4}{\emph {Learning a null model}. Models and data comparison using molecule pair count statistics (example donor: S2; day 0-day 0 comparison). The learned model for Poisson distributed $P(n|f)$ (left) and for $P(n|f)$ set as a negative binomial distribution in cell counts controlling the scale parameter of a Poisson distribution of molecule counts (center; fig.\ref {fig:modelstruct}). Right: empirical pair count histogram}{figure.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Inference of diversity \& fraction observed}{4}{section*.14}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  \emph  {Learned null model parameters} plotted separately for each donor and time point. Error bars are the inverse standard deviation of a Gaussian approximation around the maximum of the likelihood acting as a lower bound for the variance of the estimates. }}{5}{figure.3}}
\newlabel{fig:nullparas_timeseries}{{3}{5}{\emph {Learned null model parameters} plotted separately for each donor and time point. Error bars are the inverse standard deviation of a Gaussian approximation around the maximum of the likelihood acting as a lower bound for the variance of the estimates}{figure.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Pushing the null model into the temporal domain}{5}{section*.15}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  \emph  {Null model marginals and conditionals}. The marginal, $P(n_1|\theta _n)=\DOTSB \sum@ \slimits@ _{n_2}P(n_1,n_2|\theta _n)$ (a), and conditional $P(n_1|n_2\theta _n)=P(n_1,n_2|\theta _n)/P(n_1|\theta _n)$ (b), distributions.{\color  {red} Add conditional $P(n|n'=0)$ and $P(n'|n=0)$. clean up figure (remove grey etc.)} }}{5}{figure.4}}
\newlabel{fig:modelfit}{{4}{5}{\emph {Null model marginals and conditionals}. The marginal, $P(n_1|\theta _n)=\sum _{n_2}P(n_1,n_2|\theta _n)$ (a), and conditional $P(n_1|n_2\theta _n)=P(n_1,n_2|\theta _n)/P(n_1|\theta _n)$ (b), distributions.{\color {red} Add conditional $P(n|n'=0)$ and $P(n'|n=0)$. clean up figure (remove grey etc.)}}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  \emph  {Diversity estimates.} Shown are diversity estimates obtained from the Renyi entropies, $H_\beta $, of the inferred clone frequency distributions for $\beta =0$ (estimated total number of clones, $N$), $\beta =1$ (Shannon entropy) and $\beta =2$ (Simpson index), across donors and days. }}{5}{figure.5}}
\newlabel{fig:div_estimates}{{5}{5}{\emph {Diversity estimates.} Shown are diversity estimates obtained from the Renyi entropies, $H_\beta $, of the inferred clone frequency distributions for $\beta =0$ (estimated total number of clones, $N$), $\beta =1$ (Shannon entropy) and $\beta =2$ (Simpson index), across donors and days}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Differential expression model}{6}{section*.16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Model description}{6}{section*.17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Example 1: base prior and mouse vs. human repertoires}{6}{section*.18}}
\newlabel{eq:Ps_ex1}{{10}{6}{}{equation.0.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Learning $\rho (s)$ on real data}{6}{section*.19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Inferring global properties of the differentially expressed repertoire}{7}{section*.20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Prior solvable via expectation maximization}{7}{section*.21}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  \emph  {Inference on synthetic data.}(a) $\rho (s)$, eq. \ref  {eq:Ps_ex1}, is parametrized by an effect size, $\mathaccentV {bar}016{s}$, describing the expansion of the responding fraction, $\alpha $ of the repertoire. Expansion is relative to the functions center, $s_0$, which is fixed by the homeostatic constraint $\delimiter "426830A f \delimiter "526930B =\delimiter "426830A f' \delimiter "526930B $. (b) Inferring $\theta ^*=(\mathaccentV {bar}016{s}^*,\alpha ^* )=(1.0,10^{-2})$ (black dot). Maximums of the log-likelihood, $\ell _\textrm  {max}=\ell (\mathaccentV {hat}05E{\theta }_r)$, for many realizations, $r=1,\dots  ,50$, are given by gray crosses, with their average, $\mathaccentV {bar}016{\mathaccentV {hat}05E{\theta }}$, shown as the black cross. The log-likelihood, $\ell (\theta )$, for one realization is shown over logarithmically-spaced gray contours decreasing from the maximum, $\ell _{\textrm  {max}}$. The inverse Fisher information, $\mathcal  {I}^{-1}$, for a realization is shown as the black-lined ellipse centered around its maximum, $\mathaccentV {bar}016{\mathaccentV {hat}05E{\theta }}$, provides a lower bound to the variance of our ML estimate. The gray scale contours increasing to the upper-right denote the excess in the used normalization, $Z=e^{s_0}$, above $1$. (c) Posteriors of the learned model, $P(s|n,n')$ over pairs $(n,n')$ for $n'=n$, with $n$ varying over a logarithmically-spaced set of counts (left), and for $n'$ given by the reverse order of this set (right). The black dot in both plots denotes the $1-\alpha $ non-responding component, $\delta (s-s_0)$. (Parameters: $N=10^6$,$\epsilon =10^-2$.) }}{7}{figure.6}}
\newlabel{fig:diffexpr_ex1}{{6}{7}{\emph {Inference on synthetic data.}(a) $\rho (s)$, eq. \ref {eq:Ps_ex1}, is parametrized by an effect size, $\bar {s}$, describing the expansion of the responding fraction, $\alpha $ of the repertoire. Expansion is relative to the functions center, $s_0$, which is fixed by the homeostatic constraint $\langle f \rangle =\langle f' \rangle $. (b) Inferring $\theta ^*=(\bar {s}^*,\alpha ^* )=(1.0,10^{-2})$ (black dot). Maximums of the log-likelihood, $\ell _\textrm {max}=\ell (\hat {\theta }_r)$, for many realizations, $r=1,\dots ,50$, are given by gray crosses, with their average, $\bar {\hat {\theta }}$, shown as the black cross. The log-likelihood, $\ell (\theta )$, for one realization is shown over logarithmically-spaced gray contours decreasing from the maximum, $\ell _{\textrm {max}}$. The inverse Fisher information, $\mathcal {I}^{-1}$, for a realization is shown as the black-lined ellipse centered around its maximum, $\bar {\hat {\theta }}$, provides a lower bound to the variance of our ML estimate. The gray scale contours increasing to the upper-right denote the excess in the used normalization, $Z=e^{s_0}$, above $1$. (c) Posteriors of the learned model, $P(s|n,n')$ over pairs $(n,n')$ for $n'=n$, with $n$ varying over a logarithmically-spaced set of counts (left), and for $n'$ given by the reverse order of this set (right). The black dot in both plots denotes the $1-\alpha $ non-responding component, $\delta (s-s_0)$. (Parameters: $N=10^6$,$\epsilon =10^-2$.)}{figure.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Posteriors of log fold-change}{8}{section*.22}}
\@writefile{toc}{\contentsline {section}{\numberline {I}Rest is work in progress}{8}{section*.23}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  \emph  {Supp. Fig: Self-consistent reinference of diffexpr model}. }}{8}{figure.7}}
\newlabel{fig:suppfig2_reinf_diffexpr}{{7}{8}{\emph {Supp. Fig: Self-consistent reinference of diffexpr model}}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  \emph  {Analysis of sloppiness of model: description of max-Likelihood manifold and possibly reduced description} }}{8}{figure.8}}
\newlabel{fig:Data}{{8}{8}{\emph {Analysis of sloppiness of model: description of max-Likelihood manifold and possibly reduced description}}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  \emph  {Evolution of parameters}. }}{8}{figure.9}}
\newlabel{fig:timeevo}{{9}{8}{\emph {Evolution of parameters}}{figure.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Ensemble-level application: Time-tracking of ensemble parameters}{8}{section*.24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Clone-level application: Identification of responding clones}{8}{section*.25}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  \emph  {Posteriors}. Some example posteriors. Distributions of slow, smed, shigh, and Pval. Volcano plot. }}{8}{figure.10}}
\newlabel{fig:posteriors}{{10}{8}{\emph {Posteriors}. Some example posteriors. Distributions of slow, smed, shigh, and Pval. Volcano plot}{figure.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}discussion}{9}{section*.26}}
\@writefile{toc}{\contentsline {section}{\numberline {}Methods}{9}{section*.27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Null model definition}{9}{section*.28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Normalization}{9}{section*.29}}
\newlabel{eq:bigZ}{{22}{10}{}{equation.1.22}{}}
\newlabel{eq:largedev}{{23}{10}{}{equation.1.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Obtaining diversity estimates from the clone frequency density}{10}{section*.30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Null Model Sampling}{10}{section*.31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Null Model Inference}{10}{section*.32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Differential expression model definition}{11}{section*.33}}
\newlabel{eq:genPs}{{28}{11}{}{equation.1.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Identifying responding clones}{11}{section*.34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Alternative model sampling procedure}{11}{section*.35}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces  \emph  {Reinferring null model parameters}. Shown are the actual and estimated values of the null model parameters used to validate the null model inference procedure over the range exhibited by the data. A 3x3x3x3 grid of points were sampled and results collapsed over each parameter axis. $f_{min}$ was fixed to satisfy the normalization constraint. }}{11}{figure.11}}
\newlabel{fig:reinfer_null}{{11}{11}{\emph {Reinferring null model parameters}. Shown are the actual and estimated values of the null model parameters used to validate the null model inference procedure over the range exhibited by the data. A 3x3x3x3 grid of points were sampled and results collapsed over each parameter axis. $f_{min}$ was fixed to satisfy the normalization constraint}{figure.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Direct Sampling}{11}{section*.36}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Effective Sampling}{11}{section*.37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}equal frequency constraint}{12}{section*.38}}
\@writefile{toc}{\contentsline {section}{\numberline {}Acknowledgements}{12}{section*.39}}
\@writefile{toc}{\contentsline {section}{\numberline {}Author Contributions}{12}{section*.40}}
\@writefile{toc}{\contentsline {section}{\numberline {}Additional Information}{12}{section*.41}}
\@writefile{toc}{\appendix }
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendixes}{12}{section*.42}}
\bibdata{diffexpr_MAIN_TEXTNotes,diffexpr}
\bibcite{Robinson2008}{{1}{2008}{{Robinson\ and\ Smyth}}{{}}}
\bibcite{Best2015a}{{2}{2015}{{Best\ \emph  {et~al.}}}{{Best, Oakes, Heather, Shawe-Taylor,\ and\ Chain}}}
\bibcite{Mora2016}{{3}{2016}{{Mora\ and\ Walczak}}{{}}}
\bibstyle{apsrev4-1}
\citation{REVTEX41Control}
\citation{apsrev41Control}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{13}{section*.43}}
\newlabel{LastBibItem}{{3}{13}{}{section*.43}{}}
\newlabel{LastPage}{{}{13}{}{}{}}
