\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\newlabel{FirstPage}{{}{1}{}{section*.1}{}}
\@writefile{toc}{\contentsline {title}{Inferring repertoire dynamics from repertoire sequencing}{1}{section*.2}}
\@writefile{toc}{\contentsline {abstract}{Abstract}{1}{section*.1}}
\@writefile{toc}{\contentsline {section}{\numberline {}Model family}{1}{section*.3}}
\newlabel{eq:jointf}{{1}{1}{}{equation.0.1}{}}
\citation{Robinson2008}
\newlabel{eq:norm_constr}{{2}{2}{}{equation.0.2}{}}
\newlabel{eq:orignorm}{{4}{2}{}{equation.0.4}{}}
\newlabel{eq:postnorm}{{5}{2}{}{equation.0.5}{}}
\newlabel{eq:fprimeconst}{{6}{2}{}{equation.0.6}{}}
\citation{Robinson2008}
\citation{Best2015a}
\@writefile{toc}{\contentsline {section}{\numberline {}Results}{3}{section*.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}A null model for replicate clone size variation}{3}{section*.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  \emph  {Repertoire model}. (a) Clone frequency distribution, $\rho (f)$, is set as a power law, parameterized by the power, $\alpha $ and the minimum frequency, $f_{min}$. (b) (c) Differential expression model structure and learning procedure. Null model parameters are learned by marginalizing over clone frequency, $f$, and maximizing this marginal likelihood with respect to the parameters. Sampled repertoires can then be generated using the ML estimate. $P(n|f)$ is set as a negative binomial distribution in cell counts controlling the scale parameter of a Poisson distribution of molecule counts. In the differentially expressed condition (prime-decorated quantities), the parameters, $\theta _{diff}$, of the prior distribution, $\rho (c)$, of fold-change, $c$, are learned by maximizing the marginal likelihood (see section \textit  {Fold change prior}) with respect to $\theta _{diff}$, keeping the remaining parameters, $\theta _{null}$, fixed to their ML estimates, $\mathaccentV {hat}05E{\theta }_{null}$, previously obtained using same-day replicate data (see fig. \ref  {fig:nullstats}). }}{3}{figure.1}}
\newlabel{fig:fullmodel}{{1}{3}{\emph {Repertoire model}. (a) Clone frequency distribution, $\rho (f)$, is set as a power law, parameterized by the power, $\alpha $ and the minimum frequency, $f_{min}$. (b) (c) Differential expression model structure and learning procedure. Null model parameters are learned by marginalizing over clone frequency, $f$, and maximizing this marginal likelihood with respect to the parameters. Sampled repertoires can then be generated using the ML estimate. $P(n|f)$ is set as a negative binomial distribution in cell counts controlling the scale parameter of a Poisson distribution of molecule counts. In the differentially expressed condition (prime-decorated quantities), the parameters, $\theta _{diff}$, of the prior distribution, $\rho (c)$, of fold-change, $c$, are learned by maximizing the marginal likelihood (see section \textit {Fold change prior}) with respect to $\theta _{diff}$, keeping the remaining parameters, $\theta _{null}$, fixed to their ML estimates, $\hat {\theta }_{null}$, previously obtained using same-day replicate data (see fig. \ref {fig:nullstats})}{figure.1}{}}
\citation{Mora2016}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  \emph  {Learning a null model}. Models and data comparison using molecule pair count statistics (example donor: S2; day 0-day 0 comparison). The learned model for Poisson distributed $P(n|f)$ (left) and for $P(n|f)$ set as a negative binomial distribution in cell counts controlling the scale parameter of a Poisson distribution of molecule counts (center; fig.\ref  {fig:modelstruct}). Right: empirical pair count histogram. }}{4}{figure.2}}
\newlabel{fig:nullstats}{{2}{4}{\emph {Learning a null model}. Models and data comparison using molecule pair count statistics (example donor: S2; day 0-day 0 comparison). The learned model for Poisson distributed $P(n|f)$ (left) and for $P(n|f)$ set as a negative binomial distribution in cell counts controlling the scale parameter of a Poisson distribution of molecule counts (center; fig.\ref {fig:modelstruct}). Right: empirical pair count histogram}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  \emph  {Learned null model parameters} plotted separately for each donor and time point. Error bars are the inverse standard deviation of a Gaussian approximation around the maximum of the likelihood acting as a lower bound for the variance of the estimates. }}{4}{figure.3}}
\newlabel{fig:nullparas_timeseries}{{3}{4}{\emph {Learned null model parameters} plotted separately for each donor and time point. Error bars are the inverse standard deviation of a Gaussian approximation around the maximum of the likelihood acting as a lower bound for the variance of the estimates}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  \emph  {Null model marginals and conditionals}. The marginal, $P(n_1|\theta _n)=\DOTSB \sum@ \slimits@ _{n_2}P(n_1,n_2|\theta _n)$ (a), and conditional $P(n_1|n_2\theta _n)=P(n_1,n_2|\theta _n)/P(n_1|\theta _n)$ (b), distributions.{\color  {red} Add conditional $P(n|n'=0)$ and $P(n'|n=0)$. clean up figure (remove grey etc.)} }}{5}{figure.4}}
\newlabel{fig:modelfit}{{4}{5}{\emph {Null model marginals and conditionals}. The marginal, $P(n_1|\theta _n)=\sum _{n_2}P(n_1,n_2|\theta _n)$ (a), and conditional $P(n_1|n_2\theta _n)=P(n_1,n_2|\theta _n)/P(n_1|\theta _n)$ (b), distributions.{\color {red} Add conditional $P(n|n'=0)$ and $P(n'|n=0)$. clean up figure (remove grey etc.)}}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  \emph  {Diversity estimates.} Shown are diversity estimates obtained from the Renyi entropies, $H_\beta $, of the inferred clone frequency distributions for $\beta =0$ (estimated total number of clones, $N$), $\beta =1$ (Shannon entropy) and $\beta =2$ (Simpson index), across donors and days. }}{5}{figure.5}}
\newlabel{fig:div_estimates}{{5}{5}{\emph {Diversity estimates.} Shown are diversity estimates obtained from the Renyi entropies, $H_\beta $, of the inferred clone frequency distributions for $\beta =0$ (estimated total number of clones, $N$), $\beta =1$ (Shannon entropy) and $\beta =2$ (Simpson index), across donors and days}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Differential expression model}{5}{section*.6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}model description}{5}{section*.7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Example 1: base prior and mouse repertoire}{6}{section*.8}}
\newlabel{eq:Ps_ex1}{{10}{6}{}{equation.0.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Prior solvable via expectation maximization}{6}{section*.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  \emph  {Inference on synthetic data.}(a) $\rho (s)$, eq. \ref  {eq:Ps_ex1}, is parametrized by an effect size, $\mathaccentV {bar}016{s}$, describing the expansion of the responding fraction, $\alpha $ of the repertoire. Expansion is relative to the functions center, $s_0$, which is fixed by the homeostatic constraint $\delimiter "426830A f \delimiter "526930B =\delimiter "426830A f' \delimiter "526930B $. (b) Inferring $\theta ^*=(\mathaccentV {bar}016{s}^*,\alpha ^* )=(1.0,10^{-2})$ (black dot). Maximums of the log-likelihood, $\ell _\textrm  {max}=\ell (\mathaccentV {hat}05E{\theta }_r)$, for many realizations, $r=1,\dots  ,50$, are given by gray crosses, with their average, $\mathaccentV {bar}016{\mathaccentV {hat}05E{\theta }}$, shown as the black cross. The log-likelihood, $\ell (\theta )$, for one realization is shown over logarithmically-spaced gray contours decreasing from the maximum, $\ell _{\textrm  {max}}$. The inverse Fisher information, $\mathcal  {I}^{-1}$, for a realization is shown as the black-lined ellipse centered around its maximum, $\mathaccentV {bar}016{\mathaccentV {hat}05E{\theta }}$, provides a lower bound to the variance of our ML estimate. The gray scale contours increasing to the upper-right denote the excess in the used normalization, $Z=e^{s_0}$, above $1$. (c) Posteriors of the learned model, $P(s|n,n')$ over pairs $(n,n')$ for $n'=n$, with $n$ varying over a logarithmically-spaced set of counts (left), and for $n'$ given by the reverse order of this set (right). The black dot in both plots denotes the $1-\alpha $ non-responding component, $\delta (s-s_0)$. (Parameters: $N=10^6$,$\epsilon =10^-2$.) }}{6}{figure.6}}
\newlabel{fig:diffexpr_ex1}{{6}{6}{\emph {Inference on synthetic data.}(a) $\rho (s)$, eq. \ref {eq:Ps_ex1}, is parametrized by an effect size, $\bar {s}$, describing the expansion of the responding fraction, $\alpha $ of the repertoire. Expansion is relative to the functions center, $s_0$, which is fixed by the homeostatic constraint $\langle f \rangle =\langle f' \rangle $. (b) Inferring $\theta ^*=(\bar {s}^*,\alpha ^* )=(1.0,10^{-2})$ (black dot). Maximums of the log-likelihood, $\ell _\textrm {max}=\ell (\hat {\theta }_r)$, for many realizations, $r=1,\dots ,50$, are given by gray crosses, with their average, $\bar {\hat {\theta }}$, shown as the black cross. The log-likelihood, $\ell (\theta )$, for one realization is shown over logarithmically-spaced gray contours decreasing from the maximum, $\ell _{\textrm {max}}$. The inverse Fisher information, $\mathcal {I}^{-1}$, for a realization is shown as the black-lined ellipse centered around its maximum, $\bar {\hat {\theta }}$, provides a lower bound to the variance of our ML estimate. The gray scale contours increasing to the upper-right denote the excess in the used normalization, $Z=e^{s_0}$, above $1$. (c) Posteriors of the learned model, $P(s|n,n')$ over pairs $(n,n')$ for $n'=n$, with $n$ varying over a logarithmically-spaced set of counts (left), and for $n'$ given by the reverse order of this set (right). The black dot in both plots denotes the $1-\alpha $ non-responding component, $\delta (s-s_0)$. (Parameters: $N=10^6$,$\epsilon =10^-2$.)}{figure.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Posteriors of log fold-change}{7}{section*.10}}
\@writefile{toc}{\contentsline {section}{\numberline {I}Rest is work in progress}{7}{section*.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  \emph  {Supp. Fig: Self-consistent reinference of diffexpr model}. }}{7}{figure.7}}
\newlabel{fig:suppfig2_reinf_diffexpr}{{7}{7}{\emph {Supp. Fig: Self-consistent reinference of diffexpr model}}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  \emph  {Analysis of sloppiness of model: description of max-Likelihood manifold and possibly reduced description} }}{7}{figure.8}}
\newlabel{fig:Data}{{8}{7}{\emph {Analysis of sloppiness of model: description of max-Likelihood manifold and possibly reduced description}}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  \emph  {Evolution of parameters}. }}{7}{figure.9}}
\newlabel{fig:timeevo}{{9}{7}{\emph {Evolution of parameters}}{figure.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Ensemble-level application: Time-tracking of ensemble parameters}{7}{section*.12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Clone-level application: Identification of responding clones}{7}{section*.13}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  \emph  {Posteriors}. Some example posteriors. Distributions of slow, smed, shigh, and Pval. Volcano plot. }}{7}{figure.10}}
\newlabel{fig:posteriors}{{10}{7}{\emph {Posteriors}. Some example posteriors. Distributions of slow, smed, shigh, and Pval. Volcano plot}{figure.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}discussion}{8}{section*.14}}
\@writefile{toc}{\contentsline {section}{\numberline {}Methods}{8}{section*.15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Null model definition}{8}{section*.16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Normalization}{8}{section*.17}}
\newlabel{eq:bigZ}{{21}{9}{}{equation.1.21}{}}
\newlabel{eq:largedev}{{22}{9}{}{equation.1.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Obtaining diversity estimates from the clone frequency density}{9}{section*.18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Null Model Sampling}{9}{section*.19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Null Model Inference}{9}{section*.20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Differential expression model definition}{10}{section*.21}}
\newlabel{eq:genPs}{{27}{10}{}{equation.1.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Identifying responding clones}{10}{section*.22}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces  \emph  {Reinferring null model parameters}. Shown are the actual and estimated values of the null model parameters used to validate the null model inference procedure over the range exhibited by the data. A 3x3x3x3 grid of points were sampled and results collapsed over each parameter axis. $f_{min}$ was fixed to satisfy the normalization constraint. }}{10}{figure.11}}
\newlabel{fig:reinfer_null}{{11}{10}{\emph {Reinferring null model parameters}. Shown are the actual and estimated values of the null model parameters used to validate the null model inference procedure over the range exhibited by the data. A 3x3x3x3 grid of points were sampled and results collapsed over each parameter axis. $f_{min}$ was fixed to satisfy the normalization constraint}{figure.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Alternative model sampling procedure}{10}{section*.23}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Direct Sampling}{10}{section*.24}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Effective Sampling}{10}{section*.25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}equal frequency constraint}{11}{section*.26}}
\@writefile{toc}{\contentsline {section}{\numberline {}Acknowledgements}{11}{section*.27}}
\@writefile{toc}{\contentsline {section}{\numberline {}Author Contributions}{11}{section*.28}}
\@writefile{toc}{\contentsline {section}{\numberline {}Additional Information}{11}{section*.29}}
\@writefile{toc}{\appendix }
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendixes}{11}{section*.30}}
\bibdata{diffexpr_MAIN_TEXTNotes,diffexpr}
\bibcite{Robinson2008}{{1}{2008}{{Robinson\ and\ Smyth}}{{}}}
\bibcite{Best2015a}{{2}{2015}{{Best\ \emph  {et~al.}}}{{Best, Oakes, Heather, Shawe-Taylor,\ and\ Chain}}}
\bibcite{Mora2016}{{3}{2016}{{Mora\ and\ Walczak}}{{}}}
\bibstyle{apsrev4-1}
\citation{REVTEX41Control}
\citation{apsrev41Control}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{12}{section*.31}}
\newlabel{LastBibItem}{{3}{12}{}{section*.31}{}}
\newlabel{LastPage}{{}{12}{}{}{}}
