\documentclass[letterpaper,english,prl,reprint,longbibliography]{revtex4-1} %twocolumn,

%\usepackage[latin9]{inputenc}
\setcounter{secnumdepth}{3}
\usepackage{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx} 
\usepackage{epstopdf} %add for pdflatex, nut don^{\prime}t compile because of invisible character copied from .bbl file?
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc} 
\usepackage{esint}
\usepackage{verbatim}
%\usepackage[hyphens]{url}
\usepackage[unicode=true]{hyperref}
\usepackage[capitalize,nameinlink]{cleveref}
\creflabelformat{equation}{#2\textup{#1}#3}
\usepackage[table]{xcolor}
\newcommand{\re}[1]{\textcolor{red}{#1}}
%\loadpackage{url}
%\usepackage[unicode=true]{hyperref}
%\setcounter{biburllcpenalty}{7000}
%\setcounter{biburlucpenalty}{8000}
%\usepackage{breakurl}
%\hypersetup{breaklinks=true}
%\usepackage[hyperref,eprint=false]{biblatex}
\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\pdfpageheight\paperheight
\pdfpagewidth\paperwidth

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{bbold}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\makeatother

\begin{document}

\preprint{APS/123-QED}

\title{Inferring repertoire dynamics from repertoire sequencing}

\author{Maximilian Puelma Touzel}
%\email[]{puelma@lpt.ens.fr}

\affiliation{Laboratoire de Physique Théorique, ENS-PSL Research University, Paris, France}
\affiliation{Mila, Université de Montréal, Montreal, Canada}

\author{Aleksandra Walczak}

\affiliation{Laboratoire de Physique Théorique, ENS-PSL Research University, Paris, France}

\author{Thierry Mora}

\affiliation{Laboratoire de Physique Statistique, ENS-PSL Research University, Paris, France,}

\vspace{0.5cm}

\begin{abstract}
\re{Good, but need to add new, main takeaways.} High-throughput sequencing provides access to expression-level detail of cell populations. Identifying signal in this data is nevertheless a challenge: intrinsic variability, vast sub-sampling, and indirect access together make difficult the reliable and accurate inference of the changes in population size due to environmental perturbations. Here, in the context of antigen-perturbed immune cell repertoires, we formulate access to the unobserved repertoire using a generative model of observed sequence count pairs from a reference and differentially-expressed condition. When applied to pairs of replicates, our model captures the natural variability in the system, giving reproducible behavior across donors and, in spite of sub-sampling, provides plausible parameter estimates for the underlying repertoire. Using this replicate model as a baseline, we then formulate the differentially expressed condition using a prior distribution on the ratio of a clone’s frequency pair for pairs of repertoires sampled at different time points. The posterior distribution of this ratio for each observed clone is then obtained and used to characterize if and how strongly it participates in the response. Applying our approach to yellow fever vaccination as a model of acute infection in humans, we identify candidate clones participating in the response.   

\end{abstract}

\keywords{keywords}

\pacs{}

\maketitle
%\section{Introduction}
%\item Power-law clone size distributions are observed for sequenced uncompartmentalized repertoires. Various candidate mechanisms underlying power law (mixture of birth death etc. ref. desponds). Nevertheless, what is clear is that simple birth death alone is excluded since it gives exponential distribution that has no long, power law tail.
\input{intro}
% Background:

% Frontline ADAPTIVE immune system. $~10^{11}$ cells. $\leq 10^8$ clones. not directly observable .intrinsically variable/fluctuating
% selected repertoire sculpted by infection and vaccination history
% Its approximately $10^{11}$ lymphocytes that make up at least $10^8$ number of clones, the cells in each clone express the same receptor. 
% We don’t access the full repertoire, but only observe a small subsample of it. 
% The full repertoire and thus our samples of it vary across individuals, and even from replicates within the same individual. 
% It is also sculpted by infection and vaccination history so ...
% If we sample in time we see changes to the repertoire, and in particular as they respond to an intervening perturbation, like a vaccine. 

The paper is organized as follows. In \cref{sec:model}, we specify the generic form of the generative model of receptor RNA count pair statistics and its assumptions. Then in \cref{sec:rep_pair}, we apply it to replicate pairs to quantify the natural size variation. Next, in \cref{sec:diffexpr} we apply it to construct a differential expression model by augmenting this null model with the hidden log-frequency fold-change of clone frequency between the two compared conditions, learning a prior on such change from the data. Seeing how the parameters of this prior vary across pair time-point comparisons provides the repertoire dynamics at ensemble-level of description. We also show how our learned model can be used to infer a posterior probability distribution of fold change for any observed clone. We show that the latter can be used to infer the temporal changes of that clone's frequency. Finally, in \cref{sec:resp_clones} we use this fact to infer from the posterior expansion probability a list of clones significantly expanded by YF vaccination across a given pair of time points. 

\section*{Results}

\subsection{A repertoire model family learnable from pair RepSeq datasets}\label{sec:model}
Mathematically, dynamics in a wide variety of contexts is captured by a time-evolution operator, or propagator, $F_{t,t^{\prime}}$, that evolves the state of the system from time $t$ to time $t^{\prime}$. Our model formulation of repertoire dynamics thus focuses on characterizing the transformation of clone frequencies between a pair of time points (called reference and test). All such frequencies are unobserved, however, and so we propose a family of generative models of the count pair statistics of what is actually measured: immune receptor RNA molecules obtained by sequencing blood samples. In this case, the model parameters are constrained by the corresponding pair of measured repertoires at the reference and test times. 

Our method to determine differential expression proceeds in two steps, where in each we define, learn, and analyze an instance of this model family, first for same day replicates, then for a reference and test pair of conditions. With the latter model learned, we can arrive at estimates for quantities characterizing the repertoire as a whole, such as the total number of clones, $N$.
A second application is to use the model to make statements about individual observed clones. Namely, what can be said about changes in a clone's frequency between the two time points based on its observed molecule count pair.
All code used to produce the results in this work was custom written in Python 3 and is publically available online on \hyperlink{https://github.com/mptouzel/bayes_diffexpr}{GitHub}.
%These are the union of two sets of \emph{observed} clones, i.e. clones that are captured in the blood sample and amplified in the sequencer in either of the two conditions, providing a finite number of molecules in at least one of the reference-test pair.
%The unobserved frequencies of these observed clones directly impact the model's observed distribution, $P(n,n^{\prime})$, of molecule count pairs, $n$ and $n^{\prime}$, in the reference and test condition, respectively (\cref{fig:nullstats}A; see Methods section for details). 


\begin{figure*}[ht!]
% \includegraphics[width=\linewidth]{fig1_nullmodel}
\includegraphics[width=\linewidth]{fig1_modeldiagram}
\centering{}
\caption{
\emph{Data and modeling method}. \re{restructure} 
Left: Data sets consist of pairs of bar-coded repertoire-sequenced (RepSeq) blood samples, either from same or different days. Each data set is summarized in its pair count histogram. Right: Clone frequency distribution, $\rho(f)$, is set as a power law, parameterized by the power, $\alpha_f$ and the minimum frequency, $f_{\textrm{min}}$. With a choice of the measurement model, the null model is then specified and learned using a replicate pair data set. Using these parameters and choosing a form for the log fold-change prior, the differential expression model is specified and learned on a differentially expressed pair data set. Once learned, the model is used for posterior inference performed on all observed clones.
%(c) Differential expression model structure and learning procedure. Null model parameters are learned by marginalizing over clone frequency, $f$, and maximizing this marginal likelihood with respect to the parameters. 
%Sampled repertoires can then be generated using the ML estimate. $P(n|f)$ is set as a negative binomial distribution in cell counts controlling the scale parameter of a Poisson distribution of molecule counts. In the differentially expressed condition (prime-decorated quantities), the parameters, $\theta_{diff}$, of the prior distribution, $\rho(c)$, of fold-change, $c$, are learned by maximizing the marginal likelihood (see section \textit{Fold change prior}) with respect to $\theta_{diff}$, keeping the remaining parameters, $\theta_{null}$, fixed to their ML estimates, $\hat{\theta}_{null}$, previously obtained using same-day replicate data (see \cref{fig:nullstats}).
\label{fig:fullmodel}}
\end{figure*}

\subsubsection*{Model family definition}

We formulate an immune repertoire as a set of $N$ clone frequencies $\vec{f}=(f_1,\dots,f_N)$, each within the interval $[f_{\textrm{min}},1]$, where $f_{\textrm{min}}$ is the minimum allowed frequency of corresponding to a single lymphocyte. A prior density over clone frequencies is given by $\rho(f)$. $N$ and $f_{\textrm{min}}$ must be determined self-consistently when defining the corresponding joint density, 
\begin{eqnarray}
	\rho_N(\vec{f})\propto \prod_{i=1}^N\rho(f_i)\delta(Z_f-1)\;,\label{eq:jointf}
\end{eqnarray}
where the Dirac delta-function, $\delta(x)$, is used to impose a normalization constraint on the sum of frequencies, $Z_f=\sum_{i=1}^N f_i$, 
\begin{align}
  Z_f=1\;. \label{eq:norm_constr}
\end{align}
Accounting for normalization (see next section), the joint density over clones factorizes and so we focus on the statistics of single clones. 

Each clone's frequency pair from the reference and test conditions impacts its chance of being picked up in a realization of the acquisition process. This process consists of pair blood sampling and standard bar-coded RepSeq RNA sequencing. Post-sequencing, consensus reduction produces a list of unique RNA receptor sequences along with their abundance (i.e. molecular count) in the sample. 
We present a model, $P(n,n^{\prime},f,f')$, with $f$ and $f'$ and $n$ and $n^{\prime}$ denoting a clone's frequencies and receptor molecule counts in the reference and test condition, respectively. The model consists of priors on reference size statistics, $\rho(f)$, and the statistics of the transformed sizes, $\rho(f'|f)$. The rest of the model is an observation model of the acquisition process.

In general, repertoires are dominated in number by small clones missed in the acquisition process. Thus, in any realization, $n+n^{\prime}>0$ for only a relatively small number, $N_{\textrm{\textrm{obs}}}\ll N$, of clones, which can still be large since $N$ is typically $ 10^6$ ($10^9$) for mouse (human). These $N_{\textrm{obs}}$ \emph{observed} clones are those captured in the blood sample and amplified above detection levels in the sequencer in at least one of the test and reference conditions. We assume we have no further experimental access to the \emph{unobserved} clones that realize with $n+n^{\prime}=0$. Marginalizing over $f$ and $f'$ and conditioning on $n+n^{\prime}>0$, we obtain the model prediction for what we observe, 
\begin{align}
	P(n,n^{\prime}|n+n^{\prime}>0)=\frac{1-\delta_{n0}\delta_{n^{\prime}0}}{1-P(0,0)}P(n,n^{\prime})\;,
\end{align} 
i.e. the distribution of count pairs from observed clones, where $\delta_{ij}=1$ for $i=j$ and $0$ otherwise. The model estimate for the total number of clones is then $N=N_{\textrm{\textrm{obs}}}/(1-P(0,0))$. 

\subsubsection*{Normalization}
The $N-N_{\textrm{\textrm{obs}}}$ \emph{unobserved} clones influence the observed count statistics only via the presence of their frequencies in the two normalization constraints, $Z_f=1$ and $Z_{f'}=1$, so far unaccounted for in the model.
In \cref{sec:normal}, we show that $Z_f=1$ is implicitly satisfied if 
\begin{equation}
	N\langle f\rangle=1\;.\label{eq:orignorm}
\end{equation}
By employing this constraint, we impose the desired self-consistency between $f_\textrm{min}$ and $N$. Equivalently, it is expressed using the frequency posteriors, which we separate into unobserved and observed contributions,
\begin{align*}
	%1&=\langle Z\rangle_{\prod_i^N\rho(f_i|\mathcal{D})}\\
	%&=\sum_i^N \langle f_i\rangle_{\rho(f_i|\mathcal{D})}\\
	1&= NP(0,0) \langle f\rangle_{\rho(f|n+n^{\prime}=0)}+	N\sum_{n+n^{\prime}>0}P(n,n^{\prime})\langle f\rangle_{\rho(f|n,n^{\prime})}\;.
\end{align*}
$Z_f$ and $Z_{f'}$ are insensitive to the precise frequency values of a realized set of unobserved clones, and their average frequency is well approximated as the ensemble average in first term above. In contrast, the sum of frequencies of the observed clones might depend on the realization, especially in the case of large, outlying clones arising from power-law distributed clone sizes. This sensitivity can nevertheless be incorporated into the model using the observed dataset of pair of molecule counts, $\mathcal{D}=\{(n_i,n^{\prime}_i)\}_{i=1}^{N_{\textrm{\textrm{obs}}}}$, by using an importance sampling approximation, $\sum_{n+n^{\prime}>0}P(n,n^{\prime})\approx \frac{1}{N}\sum_{i=1}^{N_\textrm{obs}}$, so that the second term is $\sum_{i=1}^{N_{\textrm{obs}}}\langle f\rangle_{\rho(f|n_i,n^{\prime}_i)}$. We thus define the right-hand side of this realization-dependent constraint 
\begin{align}
	%1&=\langle Z\rangle_{\prod_i^N\rho(f_i|\mathcal{D})}\\
	%&=\sum_i^N \langle f_i\rangle_{\rho(f_i|\mathcal{D})}\\
	Z^\mathcal{D}_f&= N	P(0,0)\langle f\rangle_{\rho(f|n+n^{\prime}=0)} + \sum_{i=1}^{N_{\textrm{obs}}}\langle f\rangle_{\rho(f|n_i,n^{\prime}_i)}\;,\label{eq:postnorm}
\end{align}
and impose that $Z^\mathcal{D}_f=1$, in addition to $N\langle f\rangle=1$. We note that while not equivalent, differences in values of parameters learned with each constraint separately were small, suggesting there is a high overlap in the respective regions of the parameter space satisfying the original  \cref{eq:orignorm} constraint and realization-dependent \cref{eq:postnorm} constraint (\cref{fig:SM_match_null_constr}). We impose the same constraint on $\vec{f}'$, via the equivalent condition, 
\begin{align}
	Z^\mathcal{D}_{f'}=Z^\mathcal{D}_f\;.\label{eq:fprimeconst}
\end{align}

% Sampling from the models would be more aligned with the inference if $N_{\textrm{obs}}$ was a parameter. In that case, we need only sample the observable clones.
%$\langle Z \rangle_{\rho(\vec{f})}}$
%As a result the joint frequency factorizes and $\langle Z\rangle_{\rho(f)}\approx N\langle f\rangle_{\rho(f)}$ (same for $Z'$).
%Thus, in addition to these two constraints, choosing $N$, and the parameters of $\rho(f)$, and $\rho(f')$ to be mutually consistent demands the constraint that $N\langle f\rangle_{\rho(f)}=1$ and $ N\langle f'\rangle_{\rho(f')}=1$, respectively (the latter can be equivalently expressed as $\langle f'\rangle_{\rho(f')}=\langle f\rangle_{\rho(f)}$). For arbitrary test condition, the latter average isn^{\prime}t necessarily defined. implies that $Z$ and $Z'$ are near unity. When sampling differential expression models, we must also normalize $f'$ by $Z'$.
%As a result, 
%When inferring models, we impose the constraint $N\langle f\rangle_{\rho(f)}=1$ the same normalization, but conditioned on the observed data 
%\begin{equation}
%	1=N\langle f\rangle_{\rho(f|\mathcal{D})}
%							   = P(0,0)N\langle f\rangle_{\rho(f|n+n^{\prime}=0)} + \sum_{i}^{N_{\textrm{obs}}}\langle f\rangle_{\rho(f|n_i,n^{\prime}_i)}\;.
%\end{equation}
%and similarly for $f'$. Does this reduce to $N\langle f\rangle_{\rho(f)}=1$?

%sampling section:
%Sampling from the models would be more aligned with the inference if $N_{\textrm{obs}}$ was a parameter. In that case, we need only sample the observable clones.
%For differential expression models With $N$ a fixed input parameter to the sampling procedure, sampled frequencies are simply normalized by dividing by $Z$. For the differential expression model, we in addition impose that $\langle f\rangle=\langle f'\rangle$.


%In this case, the normalization when sampling from the model is implementation of the normalization depends on whether sampling from or inferring the parameters of the model, and also whether we are considering the null or differential expression model.
%We present each form of the normalization used in its respective section.
\subsubsection*{Parameter sharing}
We take the `common dispersion' approach \citep{Robinson2008}, in which we assume that $n$ and $n^{\prime}$ are conditionally independent once the reference and test frequency are given, and that their statistics depend only implicitly on clone identity (\emph{i.e.} clonal sequence) via these frequencies.

%See Appendix for the derivation of this single clone model from the full density over the entire repertoire of all clones. \textcolor{red}{(include this?)}. 

\subsubsection*{Learning procedure}

Models were fit using a count pair dataset, $\mathcal{D}$, by maximizing the $\log$ marginal likelihood of the data, $\sum_{i=1}^{N_{\textrm{\textrm{obs}}}} \log P(n_i,n^{\prime}_i|\theta)$, over the free parameters, $\theta$, subject to the above constraints.
We proceed in two steps.
In the first step, we consider a null model in which a replicate, e.g. same-day sample, is given for the test condition. 
In this case, the reference and test frequency are the same, $\rho(f'|f)=\delta(f'-f)$, for all clones and no additional constraint for $\vec{f'}$ is needed.
The learned parameters of $\rho(f)$ and the acquisition model from this pair serves to define the baseline, e.g. pre-vaccination statistics.
In the second step, we consider a model for differential expression in which a differentially expressed condition serves as the test, e.g. the reference and test condition being pre- and post-vaccination, respectively. 
Here, the parameters of $\rho(f)$ and the acquisition model here are set to those of the null model of reference day 0. 
As a result, $Z^\mathcal{D}_f$ is not unity, but in practice we find it is close, and thus so is $Z^\mathcal{D}_{f'}$ on account of the constraint on $\vec{f'}$, \cref{eq:fprimeconst}.  
What is different here from the replicate case is $\rho(f'|f)$: the test frequency, $f'$, is obtained from a non-identity transformation of the reference frequency, $f$.  
This transformation summarizes the effect of the dynamics assumed to act on clone sizes in the period between the two sample times. 
In the absence of a strong perturbation, such as a vaccine or acute infection, this dynamics is dominated by the diffusive behavior of some stochastic population dynamics for which the transformation is given by the corresponding Green's function.  
For a strong, transient perturbation, in contrast, time-translation invariance is broken and a transformation tailored to the properties of the transient perturbation must be specified. 
In the context of immune response to yellow fever vaccination, we focus on the latter. 
%Throughout, the optimization was performed subject to the normalization constraint, $\langle \sum_i^N f_i\rangle=1$, where $N$ is the (unobserved) total number of clones in the repertoire and angle brackets denote ensemble average. 
%Bringing the average into the sum gives $N\langle f \rangle=1$, approximately satisfied for $N$ large enough that the approximation $\langle f \rangle \approx \frac{1}{N}\sum_i^{N}f_i$ becomes valid on account of the law of large numbers. 
%The latter applies here since we define $f$ over the closed interval $\left[f_{\textrm{min}},1\right]$ so that the ensemble average $\langle f \rangle=\int f\rho(f)\textrm{d}f$ always exists, even for the power law form of $\rho(f)$, which we choose throughout.
%The resulting constraint, $\langle f \rangle=(1-P(0,0))/N_{\textrm{\textrm{obs}}}$ was enforced at each step of the learning and can be viewed as restricting the family of considered models to those for which the joint density of clone frequencies factorizes. {\color{red}More here on $\vec{f}$?}.

\subsection{The replicate pair case: a null model capturing baseline clone size variation} \label{sec:rep_pair}

\subsubsection*{Replicate variability}

Using this model family, we defined a null model of count pair statistics and fit it to a pair of replicates.
This model provides a baseline variability with which differential expression can after be assessed. 
The marginal count pair distribution of this null model is  
\begin{eqnarray}
	P(n,n^{\prime}|\theta_{\textrm{null}})=\int P(n|f)P(n^{\prime}|f'=f)\rho(f)\text{d}f\;,
\end{eqnarray}
where we have collected the parameters into $\theta_{\textrm{null}}$. 
The influence of $f$ on $P(n|f)$ is an explicit parametrization  (see \cref{fig:nullstats}). For example, $P(n|f)$ as a Poisson distribution with mean proportional to $f$. 
Current methods, e.g. \cite{Robinson2008}, more accurately model $P(n|f)$ by accounting for its observed over-dispersion using a negative binomial distribution. 
The number of cells of a clone in the sample, $m$, is an additional random variable in the measurement process chain, which has so far been neglected. 
Thus, in a further refined choice for $P(n|f)$, we can explicitly account for this step by choosing $P(m|f)$ as a negative binomial distribution and then $P(n|m)$ as a Poisson distribution, giving $P(n|f)=\sum_m P(n|m)P(m|f)$. 
This two-step model, as a more explicit representation, more accurately captures the count statistics of the measurement process, especially at low counts. 
The latter fact arises from the power-law nature of frequency distribution, for which the frequency of most clones falls below the sampling depth so that the majority of clones are not captured in the sample. 
These low frequency clones are so numerous, however, that the sample is nevertheless dominated by them, each appearing at the minimum finite size. 
For a single-step model, the minimal clone count is a single molecule. 
For a two-step model, in contrast, the minimal clone size is a single cell, which gives a small, but variable number of molecules. 
The one and two-step models thus leave different signatures in the statistics at low counts, even for the additional dilution of counts due to PCR inefficiency during the sequencing of the sample \citep{Best2015a}.
We find that, indeed, this two-step model exhibits a better fit to data, especially for clones captured with few counts (see \cref{fig:SM_twostep_better}). 
We find that the fit for the over-dispersed, one-step model is not significantly worse however, while the one-step Poisson model is clearly a poor choice, as it fails to capture the over-dispersion (as visible in \cref{fig:nullstats}).


\begin{figure}[ht!]
% \includegraphics[width=\linewidth]{fig1_nullmodel}
\includegraphics{fig1_nullmodel_v5}
\centering{}
\caption{
%\emph{Learning a null model}.  Models and data comparison using molecule count pair statistics (example donor: S2; day 0-day 0 comparison). The learned model for Poisson distributed $P(n|f)$ (left) and for $P(n|f)$ set as a negative binomial distribution in cell counts controlling the scale parameter of a Poisson distribution of molecule counts (center; fig.\cref{fig:modelstruct}). Right: empirical count pair histogram.  
\emph{Measurement model comparison}. (a) The null model learning procedure. With a measurement model specified, the count pair distribution is calculated and the likelihood of the data is maximized over the parameters, $\theta_{\textrm{null}}$. (b) Data sampled from the learned models under different measurement models (left: one-step Poisson distribution, center: two-step Negative binomial distribution to Poisson distribution). Data is shown at right. (c) Parametrization of the two-step measurement model. Left: the mean-variance relationship specifying power, $\gamma$ and coefficient, $a$, of the over-dispersion. Center: the cell count distribution with mean scaling with the number of cells in the sample, $M$. Right: the molecule count distribution with mean scaling with the number of cells and the sampling efficiency, $M/N_{\textrm{r}}$, with $N_{\textrm{r}}$ the measured number of molecules in the sample.
\label{fig:nullstats}}
\end{figure}

\begin{figure}[ht!]
%\includegraphics[width=\linewidth]{fig2_learnednullparas}
\includegraphics{fig3_learnednullparas}
\centering{}
\caption{
\emph{Learned null model parameters}. Data is plotted separately for a pair of replicates across donors and time points. Error bars are obtained by projecting onto the respective parameter axis \re{(not from projecting the Fisher Information in the manifold satisfying the constraint)}. 
\label{fig:nullparas_timeseries}}
\end{figure}

\begin{figure}[ht!]
\includegraphics[width=\linewidth]{null_model_validation}
\centering{}
\caption{
\emph{Null model marginals and conditionals}. The marginal, $P(n|\theta_{\textrm{null}})=\sum_{n^{\prime}}P(n,n^{\prime}|\theta_{\textrm{null}})$ (a), and conditional $P(n|n^{\prime},\theta_{\textrm{null}})=P(n,n^{\prime}|\theta_{\textrm{null}})/P(n|\theta_{\textrm{null}})$ (b) distributions. Lines are analytic predictions of the learned model. Dots are estimated frequencies. (Donor S2, day 0/day 0 comparison).
\label{fig:modelfit}}
\end{figure}

\begin{figure}[ht!]
%\includegraphics[width=\linewidth]{fig2_learnednullparas}
\includegraphics{fig4_div_estimates}
\centering{}
\caption{
\emph{Diversity estimates.} Shown are diversity estimates obtained from the Renyi entropies, $H_\beta$, of the inferred clone frequency distributions for $\beta=0$ (estimated total number of clones, $N$), $\beta=1$ (Shannon entropy) and $\beta=2$ (Simpson index), across donors and days.
\label{fig:div_estimates}}
\end{figure}

% \begin{figure*}[ht!]
% \includegraphics[width=\textwidth]{fig2_learnednullparas}
% \centering{}
% \caption{
% \emph{Histograms of learned null model parameters}. The clone frequency distribution $\rho(f)=\int_{f_{min}}^{1}f^\alpha\textrm{d}f/Z$, is determined by the power $\alpha$ and minimal frequency, $f_{min}$. The cell count variance, $\sigma^2_m=fM+a(fM)^\gamma$, is determined by the total number of cells in the sample, $M$, and the power, $\gamma$, and coefficient, $a$, controlling the over-dispersion.{\color{red}update figure with remaining data}
% \label{fig:nullparas}}
% \end{figure*}

% Here we provide an accurate, process-based model of the observed variability of RNA count pairs obtained from pairs of same-day replicates sampled together, but sequenced separately (see  \cref{fig:nullstats}).
% %We developed four process-motivated generative models for the replicate count-pair distribution (see table \ref{Tbl1:NullModelCompare} and \cref{fig:modelstruct}). 
% Our model maps clone frequency to observed number of RNA receptor molecules, via the number of cells of that clone captured in the sample (see fig.\cref{fig:proc}). This two-step process better captures the behaviour of the clones captured with few counts (see Supp.  1). Presumably, this is because the smallest clone size in the sample is a single cell, which gives a variable number of RNA receptor molecules, whereas a single step process of RNA counts alone would give just a single count. 

\subsubsection*{Model validation}
The null models were fit by maximizing the $\log$ marginal likelihood of the data, $\sum_{i=1}^{N_{\textrm{\textrm{obs}}}} \log P(n_i,n^{\prime}_i|\theta_{\text{null}})$, over the parameters, $\theta_{\text{null}}$. 
For the two-step model, the parameters are $\theta_{\text{null}}=(\alpha_f,M,a,\gamma,f_{\textrm{min}})$, where $\alpha_f$ is the power law exponent, $M$ is the total number of cells, $a$ and $\gamma$ are the coefficient and power of the over-dispersion term in the mean variance relation of the negative binomial distribution of cells, and finally, $f_{\textrm{min}}$ is the minimum allowed clone frequency.
We developed a sampling protocol for this model (see \cref{sec:null_sampling}) and used it to validate our learning procedure (\cref{fig:SM_reinfer_null}), by verifying that it correctly learns ground truth.

Next, we assessed the ability of the two-step model to capture the measured count pair statistics of pairs of sequenced blood samples obtained from different volumes of the same sample, across days and donors. 
\Cref{fig:nullparas_timeseries} shows the learned values for 30 null models calculated from same-day replicates from 6 donors sampled over 5 time points spanning a 1.5 month period. 
While there is variability across donors and days, there is a surprising degree of regularity to the natural variability. In particular, despite estimates for $M$ and $f_\textrm{min}$ being very indirect, the learned values are within an order of magnitude of the expectation \re{references okay?}\citep{Mora2018,Mora2019,Qi2014}. 
The learned values of $M$ are consistent with rough estimates obtained from the known sample volume (personal communication, M. Pogorelyy), and the reciprocal of the learned values of $f_{\textrm{min}}$ ($10^{10}-10^{11}$) are plausible estimates for the total number of lymphocytes in the body.
The uncertainty associated with these estimates was assessed using the lower-bound provided by the curvature of the likelihood function, \emph{i.e.} the Fisher information, around the optimum and in the hyperplane locally satisfying the normalization constraint. 

In addition to verifying the self-consistency of the learning procedure, we validated the model by confirming that the learned values of the parameters provide a good fit to the count pair statistics via the model's prediction of the conditional and marginal distributions of count pairs (see \cref{fig:modelfit}). While not explicitly required by the fitting procedure, the correspondence with data is good. For example, \cref{fig:modelfit}b shows that marginal, $P(n)$, inherits the power law of the clone frequency distribution, but exhibits deviations from this law at low count number consistent with the data and the prevalence of clones in the sample with putative frequencies less than $1/N_{\textrm{\textrm{obs}}}$. We also note that the model even does well at capturing the count statistics in one condition when there are no observed counts in the other condition (see \cref{fig:modelfit}a).

\subsubsection*{Inference of diversity and fraction observed}

The learned models provide a clone frequency distribution, $\rho(f)$, that, together with the observed number of clones in one sample can be used to estimate measures of diversity of the repertoire producing that sample (procedure outlined in \cref{sec:infer_div}). In  \cref{fig:div_estimates}, we show the values, across donor and days, of the $\beta=0,1,2$ Hill diversities as three distinct diversity metrics: the species richness, i.e. the total number of clones; the Shannon diversity as the effective number of clones obtained from the Shannon entropy of $\rho(f)$, if one assumes a uniform distribution instead; and the Simpson diversity, the expected number of shared clones between two realized repertoires. \textcolor{red}{...Thierry will discuss the variability here...}. We note that diversity estimates obtained from observed species abundance are affected by statistical bias \citep{Mora2018}.

% \subsubsection*{Temporal variability}
% 
% \textcolor{red}{This paragraph is on control result showing negligible diffusive effect. I.e. looks at pre0-0 null model. Should we include or leave out?)}
% 
% Both the intrinsic and driven fluctuations of the population dynamics of immune receptor repertoires imply that clone frequencies will vary across days, even in the absence of any antigen-driven response. 
% If the timescale of this diffusive effect falls within the transient response time, the variability of count pair statistics over this window will be structured by both types of fluctuations. 
% in such a short time window, however, we cannot infer the diffusive component reliably, and we omit this component.
% Fitting the presented model, which lacks an explicit diffusive component, in the window might inaccurately attribute differential expression to what is actually natural clone size dynamics. 
% However, in the case of our yellow fever dataset, we can learn a pair of null models, each for two time points separated by the same length of time just before and after vaccination, respectively, and compare the variability of each to that of the null model previously learned from same-day replicates (see fig. X). 
% While we find that indeed the null model obtained from separated time points has higher variability, it is significantly lower than the variability of null models learned for pairs of time points with greater time separation. 
% This suggests that our inference of differential expression discussed in the next section is unbiased by the diffusive contribution to the population dynamics. 

% \begin{itemize}
% 	\item  presentation question: do we start with data description and plot first (See figure above) or immediately develop model first and only show data when showing fit?
% \end{itemize}

% \begin{table*}%[H] add [H] placement to break table across pages
% \caption{Null Model Selection. Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) for the for null models tested. Shown here are averages and standard deviations over 6 donors (n.b. donor-specific parameters are used throughout). \label{Tbl1:NullModelCompare}}
% \begin{ruledtabular}
% \begin{tabular}{c|c|c|c|c|c|c|c}
% index	& label					&parameters					& day pre0 (A/BIC)& day 0 (A/BIC) 	&day 7 (A/BIC) 	&day 15 (A/BIC) 	&day 45 (A/BIC)    	\\
% \hline
% 1 		& $\mathrm{Poisson}$ 	 			&$\gamma$,$f_{\textrm{min}}$				& 7.02e6/7.02e6				& x/x					& x/x  				& x/x  				& x/x\\
% 2 		& $\mathrm{NegBin}$				 	&$\gamma$,$a_n$,$\delta_n$,$f_{\textrm{min}}$  				& 5.854e6/5.854e6					& x/x				& x/x  				& x/x  				& x/x\\
% 3 		& $\mathrm{Poisson}\rightarrow \mathrm{NegBin}$ 	&$\gamma$,$M$,$a_n$,$\delta_n$,$f_{\textrm{min}}$	& 5.841e6/5.841e6					& x/x 				& x/x  				& x/x  				& x/x\\ 
% 4 		& $\mathrm{NegBin}\rightarrow \mathrm{Poisson}$  &$\gamma$,$M$,$a_m$,$\delta_m$,$f_{\textrm{min}}$ 		& 5.839e6/5.839e6					& x/x 				& x/x  				& x/x  				& x/x\\ 
% 
% \end{tabular}
% \end{ruledtabular}
% \end{table*}
% \begin{figure*}[ht!]
% \includegraphics{fig2_diffexprmodel_schematic}
% \centering{}
% \caption{
% \emph{Differential expression model structure and learning procedure}. $P(n|f)$ is set as a negative binomial distribution in cell counts controlling the scale parameter of a Poisson distribution of molecule counts. In the differentially expressed condition (prime-decorated quantities), the parameters, $\theta_{diff}$, of the prior distribution, $\rho(c)$, of fold-change, $c$, are learned by maximizing the marginal likelihood (see section \textit{Fold change prior}) with respect to $\theta_{diff}$, keeping the remaining parameters, $\theta_{null}$, fixed to their ML estimates, $\hat{\theta}_{null}$, previously obtained using same-day replicate data (see \cref{fig:nullstats}). 
% \label{fig:modelstruct}}
% \end{figure*}

% \begin{itemize}
% \item  A two-step, over-dispersed stochastic process most accurately captures the replicate statistics. 
% 
% \begin{itemize}
% \item  Single Pois fails qualitatively. see (Supp?)  \cref{fig:suppfig1_Poisfail}.
% \item  Single NB and Pois->NB both outperformed in AIC/BIC by NB->Pois with model accuracy dominating over complexity (see Table \ref{Tbl1:NullModelCompare}). 
% \end{itemize}
% 
% \item Quality of best fit: See  \cref{fig:modelfit}. The resulting model fits the data: not only the power law of the clone frequency distribution in the limit of large clones, but also the excess from that power law at small clone sizes due to finite sampling. It also captures the probability of pairs of counts that include a zero, i.e. that the clone was not identified in one of the samples. 
% %The model has no explicit dilution effect however, and so can not capture the overabundance of clones observed only once in the pair (i.e. where the other count is zero). (Show variance. fewer conditionals. How to show relative to Poisson? )
% 
% \item Sloppiness of parameter estimation: Show projections onto parameter axes of likelihood surface around maximum. Shows that $\gamma$ is tightly constrained. $a$ and $\delta$ tradeoff somewhat.
% \item Across day/donor variation: The natural variation models vary only systematically across days {\color{red} (check with rep size?)} , and do not depend strongly on the donor. We nevertheless use a donor-specific null model. Do time series of parameters, where result is that they are mostly flat. This would also show variability across donor.
% 
% \item Assess how well our model can fit the pre0-0 variation. Does it work? Why or why not.
% 
% \item Sampling self-consistency: Inference of sampled data is self-consistent: e.g. show over 10 random sampling of biorealistic range of values. See  \cref{fig:suppfig2_reinf_null}.
% 
% \end{itemize}

% Text:
% We assume each clone has an intrinsic frequency,f, and we treat clones only by this frequency, not their sequence.
% We begin with a given rho(f), from which we sample a frequency which controls the mean and over-dispersion in the unobserved distribution of cells in the sample, which controls the mean number of a Poisson distribution of the observed number of RNA molecules. 
% We then sample the number of cells and molecules a second time for the second replicate and collect the result in the joint count distribution, P(n1,n2), which we fit to same day joint count histogram by adjusting the parameters of all previous steps, starting with the exponent of the power law.
% The joint count distribution is poorly sampled so I’ll instead show you how it fits the factored pieces:
% The conditional probability of observing some number of counts given that the other replicate had a given count, and we see that the bulk of the distributions shifts as we condition on higher values. 
% Next is the clone size distribution that inherits the power law of the hidden clone frequency distribution, though with some modifications that our model captures. 
% Finally, I wanted to show that the model even fits the data in cases when one of the two counts is zero. 
% I want to remind you that just because one count of an observed pair is 0, doesn’t mean it wasn’t in that repertoire, it just had a frequency that was too small for it to be picked up our sample. 
% Inferring the expansion from such small clones will be unreliable because they convey relatively little about what frequency produced them: It could have been anything below the detectable threshold, which is a wide range. 
% our method should reflect this.
% A single set of parameters is learned for each donor. Nevertheless, the learned null models are still specific to the day comparison to which they are applied because the value for the total number of cells for a replicate in the model is set by $m_T=N/r_c$, where $N$ is the total number of reads for that replicate.


% \begin{figure}[h!]
% %\includegraphics{procedure.png}
% \caption{
% \emph{Supp. Fig: Self-consistent reinference of null model}.
% \label{fig:suppfig2_reinf_null}}
% \end{figure}
% 
% \begin{figure}[h!]
% %\includegraphics{suppfig1_null_model_example}
% \caption{
% \emph{Supp. Fig: Failure with Poisson model}.
% \label{fig:suppfig1_Poisfail}}
% \end{figure}

\subsection{The differentially-expressed pair case: quantifying variation from baseline} \label{sec:diffexpr}

\subsubsection*{An ensemble-level log-frequency fold-change model}

Here, we introduce a frequency transformation and leverage the learned variation from the null model to define and learn a model for differential expression. 
We set the clone frequency of the test condition as $f'=fe^s$, where $e^s$ is a multiplicative factor that we parametrize with a log-frequency fold-change, $s$ (the natural base is used mathematical convenience). 
We incorporate $s$ as an additional random variable in the variable chain of the model by providing a prior on $s$, $\rho(s)$. 
$n$ and $n^{\prime}$ are now conditionally independent given $f$ \textit{and} $s$.
The form of $\rho(s)$ depends on the application of the model. 
When used to describe a transient, selective perturbation relative to baseline, the form of $\rho(s)$ should contain a responding fraction with some effect size, alongside the non-responding component. 
The parameter values of these components can be chosen based on prior knowledge about typical sizes and fractions. 
Alternatively and best in the case of imprecise prior knowledge, $\rho(s)$ can be interpreted as part of the model and its parameter values learned directly from the data as was done with the prior on clone frequencies, $\rho(f)$ (the Empirical Bayes method). 
While the parameter were learned via gradient-based methods to maximize the likelihood, in \cref{sec:EM} we give an example of an semi-analytic approach to finding the optimum using the expectation maximization algorithm. 

The realistic range of values of the transformed frequencies in the differentially expressed condition are set by the properties of cell population dynamics.  For example, finite division rates imply that the transformed frequencies should be bounded relative to those in the reference condition.
In the absence of normalization of $f'$, they can differ drastically, depending on the form of $\rho(s)$, so here we must impose normalization as we did with $f$. We satisfy the normalization constraint on $\vec{f'}$ by introducing into the given $\rho(s)$ an additional shift parameter, $s_0$, set to satisfy  $Z^\mathcal{D}_{f'}=Z^\mathcal{D}_f$.

\subsubsection*{Model validation}

Similar to the learning of the null model, here the corresponding marginal count pair distribution is 
\begin{equation}
    \begin{split}
	P(n,n^{\prime}|\hat{\theta}_{\textrm{null}},\theta_{\textrm{diff}})=\iint P(n|f)P(n^{\prime}|f'=fe^s)\rho(f)\rho(s)\text{d}s\text{d}f\;.
	\end{split}
\end{equation}
Since the null model results demonstrate a near universal measurement model, here we fix the measurement model parameters to the values from the fitted null model, $\hat{\theta}_{\textrm{null}}$, for each donor. We also have collected the parameters of $\rho(s)$ (including the shift, $s_0$) into $\theta_{\textrm{diff}}$. 
We then maximize this marginal likelihood over $\theta_{\textrm{diff}}$ subject to the $Z^\mathcal{D}_{f'}=Z^\mathcal{D}_f$ constraint to obtain the estimate, $\hat{\theta}_{\textrm{diff}}$. 

% Thus, as in the case of the null model where we added the normalization constraint on the sum of clone frequencies, here we must also impose some normalization on the sum of differentially expressed clones frequencies. 
% There are some families of $\rho(s)$, however, for which the ensemble average $\langle f'\rangle=\langle fe^s\rangle$ does not exist, e.g. all those not suppressing $e^s$ in the ensemble average over $s>0$ (Gaussian families being a notable exception). 
% Once we condition on a given realization, however, such averages remain finite. 
% To be independent of the form of $\rho(s)$ then, we normalize conditioned on a realized repertoire when sampling, and conditioned on observed data when inferring.
% While the normalization in the inference of the null model ensures that the ensemble average of $f$ is unity, even in the case of the differential expression model, the average over a realized repertoire can vary with the realization.
% A realization-specific normalization, instead of ensuring unit sum, ensures that the averages of the two frequencies be equal, $\langle f\rangle=\langle f'\rangle$. 




\paragraph{Synthetic repertoire example}

To illustrate the behavior of the differential expression model, we present the case of a simple form for $\rho(s)$,
\begin{align}
	\rho_{s_0}(s)=\alpha\frac{1}{\bar{s}}e^{\frac{s_0-s}{\bar{s}}} \Theta(s-s_0)+(1-\alpha)\delta(s-s_0)\; \label{eq:Ps_ex1}
\end{align} 
(see \cref{fig:diffexpr_ex1}a). 
This choice describes a differentially expressed condition arising from a stimulus to which some fraction, $\alpha$, of the repertoire expands. 
Some of these clones respond strongly, most respond weakly, and all together with a characteristic effect size of log-frequency fold-change, $\bar{s}$, relative to non-responding clones in the remaining $1-\alpha$ fraction of the repertoire. 
$s_0<0$ shifts the probability mass to lower values of $s$. 
We set $s_0$ using the equal average frequency normalization constraint, ensuring that the sum of frequencies in the differentially expression condition equals that in the reference condition (for details see \cref{sec:shift_proc}).

We inferred the parameters of the model from model-sampled synthetic $(n,n^{\prime})$ data for both small (mouse-like) and large (human-like) synthetic repertoires over a range of biologically plausible parameter values. 
In \cref{fig:diffexpr_ex1}b, we show the parameter space of the inference of a mouse ($N=10^{6}$) repertoire generated with $(\bar{s}^*,\alpha^* )=(1.0,10^{-2})$, showing the errors are distributed roughly optimally in $(\bar{s},\alpha)$, i.e. with a covariance similar to the inverse of the corresponding Fisher information of one sample, as expected from the fact that the maximum likelihood estimator is efficient. The order of magnitude difference in axes ranges indicates that for this parametrization of $\rho(s)$, the Hessian of the likelihood is poorly conditioned. While second-order optimization methods are more efficient, this fact makes them ill-suited to our parametrization of the problem so we employ only first-order optimization methods for parameter learning.

For this particular mouse-like repertoire, the learned parameter estimates are imprecise due to the fact that the chosen value of $\alpha=0.01$ and approximately $10^4$ sampled clones means it is based on only tens to hundreds of responding clones. For human-sized repertoires, millions of clones are sampled making the inference much more precise (see \cref{fig:SM_reinf_diffexpr}). \re{...Need to add how many reads. Maybe exclude.}

\begin{figure}[tbph!]
\includegraphics{fig5_diffexpr_eval}
\centering{}
\caption{
\re{add $-s_0$ to x axis label}\emph{Inference on synthetic data.}(a) $\rho(s)$,  \cref{eq:Ps_ex1}, is parametrized by an effect size, $\bar{s}$, describing the expansion of the responding fraction, $\alpha$ of the repertoire. 
Expansion is relative to the functions center, $s_0$, which is fixed by the homeostatic constraint $\langle f \rangle=\langle f' \rangle$.
(b) Inferring $\theta^*=(\bar{s}^*,\alpha^* )=(1.0,10^{-2})$ (black dot). 
Maximums of the log-likelihood, $\ell_\textrm{max}=\ell(\hat{\theta}_r)$, for many realizations, $r=1,\dots,50$, are given by gray crosses, with their average, $\bar{\hat{\theta}}$, shown as the black cross. 
The log-likelihood, $\ell(\theta)$, for one realization is shown over logarithmically-spaced gray contours decreasing from the maximum, $\ell_{\textrm{max}}$. 
The inverse Fisher information, $\mathcal{I}^{-1}$, for a realization is shown as the black-lined ellipse centered around its maximum, $\bar{\hat{\theta}}$, provides a lower bound to the variance of our ML estimate. 
The gray scale contours increasing to the upper-right denote the excess in the used normalization, $Z=e^{s_0}$, above $1$. (c) Posteriors of the learned model, $P(s|n,n^{\prime})$ over pairs $(n,n^{\prime})$ for $n^{\prime}=n$, with $n$ varying over a logarithmically-spaced set of counts (left), and for $n^{\prime}$ given by the reverse order of this set (right). The black dot in both plots denotes the $1-\alpha$ non-responding component, $\delta(s-s_0)$.
(Parameters: $N=10^6$,$\epsilon=10^{-2}$.)
\label{fig:diffexpr_ex1}}
\end{figure}

\subsubsection*{Posterior analysis}
Once learned, the differential expression model provides for any observed clone the posterior distribution of log-frequency fold-change conditioned on the clone's observed count pair. 
It is calculated from the model by marginalizing $f$, and using Bayes' rule, 
\begin{align}
	P(s|n,n^{\prime})=\frac{P(n,n^{\prime}|s)\rho(s)}{P(n,n^{\prime})}\;,
\end{align}
where $P(n,n^{\prime}|s)=\int P(n|f)P(n^{\prime}|f'=fe^s)\rho(f)\textrm{d}f$. 

To illustrate the wide range of possible posterior shapes as a function of the observed count pairs, in \cref{fig:diffexpr_ex1}c, we show how the mass in the posteriors moves as we move in orthogonal directions in the space of observed count pair, $(n,n^{\prime})$. In particular, we see for example that the width of the posterior narrows when counts are both large, and that the model ascribes a fold-change of $s_0$ to clones with $n^{\prime} \lessapprox n$.

\subsubsection*{Measured repertoire example}
Here we run the above inference on actual sequences obtained from human blood samples across yellow fever vaccination. To guide the choice of prior for $s$, we plotted the histograms of the naive log-frequency fold-change $\ln n^{\prime}/n$ (for $n^{\prime},n>0$) (\cref{fig:SM_snaive_hists}). While these statistics are skewed relative to the unobserved statistics of $\ln f^{\prime}/f$ by neglecting pairs with $n^{\prime}=0$ or $n=0$, and by including the additional variability in the acquisition process, they nevertheless provide qualitative information about the underlying distribution of log-frequency fold-change. The histograms are qualitatively described as Gaussian distributions slightly offset from zero with tails between Gaussian and exponential form, depending on the day and donor (not shown). The Gaussian peaks are at least partially generated from the acquisition process. The tails are less affected by measurement noise, however, so the observed exponential form suggests right and left exponential tails for $\rho(s)$. Thus, here we add a contracting component to the $\rho(s)$ analyzed in the synthetic case above (\cref{eq:Ps_ex1}). While the contraction achieved by $s_0<0$ affects all clones, the contraction due to this new component only affects a fraction of clones. This might arise from the fluctuations of clone frequency dynamics or \re{ ...more explanation on two kinds of contraction?...} . In principle, inference using this form of $\rho(s)$ then reveals whether contraction is more homeostatic or clone-specific in nature.

Despite this added component, we can keep the number of parameters the same by setting its scale parameter equal to that of the expanding component. Since the contraction plays little role in normalization and other qualitative model features, this choice differs little from the case where the contraction scale parameter is specifically learned.  Thus, we set
\begin{align}
	\rho_{s_0}(s)=\alpha\frac{1}{2\bar{s}}e^{\frac{|s_0-s|}{\bar{s}}}+(1-\alpha)\delta(s-s_0)\;. \label{eq:Ps_ex2}
\end{align} 
We use this prior and day 0 as the reference and plot the resulting maximum likelihood parameter estimates for different test days in \cref{fig:diffexpr_ex2} and also across the 4 combinations of pair replicates from the reference and test days ($1\rightarrow 1	$, $1\rightarrow 2$, $2\rightarrow 1$, and $2\rightarrow 2$). 

There are a few remarks about how the learned values are dispersed. First, the day-0 replicates across donors gives an effect size less than the discretization (0.1), and so is effectively a Dirac delta function as expected. \re{...don't know why donor S1's values are in between...}. More generally, except for the day-0 comparison for the reason above, the learned values across donors cluster along a ridge of high likelihoods, as did the synthetic estimates (\cref{fig:diffexpr_ex1}). Consistent with the synthetic data, the uncertainty of these estimates, again quantified via the Fisher information (not shown), is relatively small on account of the large number ($\approx 10^6$) of observed clones, and orients along the ridge as expected. Replicate variability is much larger, but still confined to the ridge. The range of estimates for the effect size is relatively narrow compared with that for the fraction of responding clones that ranges over orders of magnitude across donors and even across replicates.

\subsubsection*{Imprecise estimation of ensemble-level differential expression}
Our model incorporates both observed and unobserved parts of the repertoire. Consequently, it provides estimates for global properties of the response, namely the fraction of the total number of clones, $\alpha$, that responds to the perturbation. Publicly available, state-of-the-art algorithms for differential expression also suggest an estimate for $\alpha$ from the outputted number of significantly differentially expressed clones in conjunction with some normalization. 
%Normalizing with published estimates of the total number of clones adds inaccuracy by neglecting inter-individual variability and likely under estimates $\alpha$ since many of these clones may respond but only the largest are captured in the sample. 
Normalizing by the number of observed clones (even when adjusted for the presence of large clones as is done in edgeR's TMM approach) is likely to overestimate $\alpha$ for the same reason: the sample preferentially excludes small, non-responding clones.
%\re{(how to explain why all the values are in fact lower than our model-based estimates?)}. 
% Estimates for an example donor are shown in \cref{table:alpha_est}. The EdgeR estimates cluster around $10^{-3}$ . 
% 
% \re{Is this worth doing?:}
To get a sense for the magnitude of these inaccuracies, we can apply one of these other algorithms (EdgeR) to samples from our repertoire model where we know the ground truth. As expected, such estimates are off by orders of magnitude ($\alpha^*_{EdgeR}=log-high$; actual $\alpha=0.01$). Our re-inference of course precisely learns $\alpha$ ($\alpha^*=low-high$) since it employs the model used to generate the sampled data. 

Regarding real data, even in our model-based approach, where we have learned the individual-specific variability, the method only constrains $\alpha$ to a range of values over a region in the parameter space highlighted as being consistent with the data. We conclude that, at least with the prior we have chosen, the data does not allow for a precise estimate of the responding fraction. Estimates obtained via more coarser methods should thus be interpreted with caution.



\begin{figure}[tbph!]
\includegraphics{fig7_sbaralp_symexp}
\centering{}
\caption{
\emph{Inference on actual data.} The optimal values of $alpha$ and $\bar{s}$ across donors and days \re{still missing values!}. The background heat map is a spatial correlation of the resulting list of significantly expanded clones. 
%(a) $\rho(s)$,  \cref{eq:Ps_ex2}, is again parametrized by an effect size, $\bar{s}$, describing the expansion and contraction of the responding fraction, $\alpha$ of the repertoire. 
\label{fig:diffexpr_ex2}}
\end{figure}

% \begin{table*}%[H] add [H] placement to break table across pages
% \caption{\re{fill in synthetic!} Estimates for the responding fraction for each of the 6 donors using the learned models (day 0-day 15 comparison). The last column holds the same estimates obtained from a synthetic dataset with known ground truth. \label{table:alpha_est}}
% \begin{ruledtabular}
% \begin{tabular}{c|c|c|c|c|c|c|c}
% Estimate	& S1					& S2					& P1 & P2 	& Q1 	& Q2 	& Synthetic ($\alpha=10^{-2}$)    	\\
% \hline
% model		& 0.316				 	&0.183  				& x					& 0.0976				& 0.294  				& 0.0759  				& x\\
% $N_\textrm{EdgeR}/N_{\textrm{samp}}$ 		& 1.05e-3 	&	1.04e-3& 0.736e-3					& 0.580e-3 				& 0.774e-3  				& 1.91e-3  				& x\\ 
% %$N_\textrm{EdgeR}/10^{11}$ 		& x  &x 		& x					& x 				& x  				& x  				& x\\ 
% 
% \end{tabular}
% \end{ruledtabular}
% \end{table*}

\subsection{Identifying responding clones} \label{sec:resp_clones}

A learned model of repertoire clone size statistics can be used to infer beliefs about differential expression given molecule count pairs of single observed clones. In this section we first describe the structure of these posteriors as a function of pair count, and then provide a threshold-based procedure for identifying clones with significant differential expression, i.e. significant log fold-change.  

\subsubsection*{Measured posterior distributions of log-frequency fold-change}  

We can explain the shape of the posteriors by breaking up the model components into three interpretable factors: $P(n|f)\rho(f)$, which depends only on $f$, $\rho(s)$, which depends only on $s$, and the remainder depending on both $f$ and $s$, $P(n^{\prime}|f'=fe^s)$. 
$P(n|f)\rho(f)$ contributes an exponential cutoff in $\log f$ near $n$.
$P(n^{\prime}|f')$ contributes a similar cutoff in $f'$ near $n^{\prime}$. 
$\log f$ and $s$ tradeoff in setting the value of $f'$.
Thus for a given $s$, the cutoff in $f$ shifts to lower values for larger values of $s$.
For fixed $f$, the corresponding cutoff in $s$ shifts to larger values for smaller values of $f$, until a maximum cutoff in $s$ is reached for $f=f_\textrm{min}$.
This cutoff in $s$ can more strongly bound the posterior as we consider $(n,n^{\prime})$ pairs with smaller $n/n^{\prime}$.
However, this large $s$ cutoff can be gated by $\rho(s)$, depending on the form of its expansion ($s>0$) tail. 
In fact, the form of the decay of the expansion component of $\rho(s)$ interacts with the decay of $\rho(f)$. 
Indeed, for power law $\rho(f)$, $\log f$ is distributed exponentially, so that $\log f$ and $s$ tradeoff additively, not only in determining $f'$ but also in determining $\rho(f'|f)$.
Which distribution, $\rho(f)$ or $\rho(s)$, dominates the shape of the posteriors then depends on the relative magnitude of their scale parameters. 

We computed the posteriors over all clones. For clones observed with $(0,n^{\prime})$ with $n^{\prime}$ large, the posteriors exhibit a large $s$ mode whose form has a maximum at a high or low positive $s$ value depending on a competition between the two priors controlled by $\alpha_f$ that preferences high $s$ and $\bar{s}$ that preferences low $s$ (see \cref{fig:posteriors}). The average posterior reflects this, through its deviation from the prior in the large, positive $s$ regime.

The structure of the contribution of singleton clones (the vast majority of all clones in the sample) reflect the balance between of the power of the power-law form of clone frequency density preferencing near maximal expansion and our conservative choice of prior preferencing expansion of a characteristic size. The uncertainty in the inferred expansion obtained from these posteriors is dispersed roughly uniform over the range of expansion for singleton clones appearing in the differentially expressed condition. 

\begin{figure}[tbph!]
%\includegraphics{volcano}

%\centering{}
\caption{
\emph{Competition between $\alpha_f$ and $\bar{s}$ in shaping the posteriors, $\rho(s|n,n^\prime)$}.\re{insert figure here} 
\label{fig:posteriors}}
\end{figure}

How do these posterior beliefs about log fold-change compare to the naive estimate, $s_{\textrm{naive}}=\ln n^{\prime}/n$É We show this estimate in \cref{fig:SM_smed_snaive}) as a function of a natural estimate from our model, the posterior median. $s_{\textrm{naive}}$ reliably matches the median when both $n$ and $n^{\prime}$ are large across a range of values of the median. This match demonstrates that our learning procedure identified parameters that respect the reliability of the fold change observed of large clones. The estimator increasingly over-estimates in magnitude where both fold change and counts are small. Indeed, where $n$ and $n^{\prime}$ are small, a large fold change is more likely due to stochastic fluctuations in realization. Our posterior inference procedure averages out these fluctuations and so in principle gives a more accurate estimate. We confirmed this using synthetic data (not shown). 


\subsubsection*{Identifying responding clones}
A significance criterion on the inferred posteriors of log-frequency fold-change can be used to identify candidate clones that participate significantly in a response. In analogy with $p$-values, we use the posterior probability corresponding to the null hypothesis that they are not expanded, $P(s\leq 0|n,n^{\prime})$ and set a threshold of significance $P_{\textrm{null}}\leq 0.025$. In \cref{fig:volcano}, we show the results as a function of posterior effect size. The threshold in confidence sets a threshold in molecular count pair space $(n,n^\prime)$. 
The structure of the data in the plot recapitulates the structure of the posterior beliefs. Focusing on expanding clones, the outer most line corresponds to clones with $(0,n^{\prime})$ count pairs with $n^{\prime}=1,2,\dots$. The next furthest is for $n=1$, where the effect size at which the response passes the threshold criteria is less. More generally, the larger $n$, the smaller is the lower bound on $n^{\prime}$ required for significance. The situation for significant contraction is analogous. Mapping the threshold back in $(n,n^\prime)$ space gives a simple prescription for selection from the count pair histogram (\cref{fig:volcano} inset).

Given the breadth of learned values for the parameters of $\rho(s)$, an natural question is how robust is the above significance procedure to that variation. In \cref{fig:table_list}, we show the list overlap as a function of $(\bar{s}, \alpha)$. There is a ridge of high overlap values mirroring the high ridge of likelihoods on which we find most learned parameters. Showing the replicates for one donor shows that indeed these different values for $\bar{s}$ and especially $\alpha$ lead to virtually identical lists of candidates for response, confirming the robustness of the latter.

\re{A final result will go here that uses the table to say something interesting. Candidates: correlation between precursor $f$ and $s_{\textrm{median}}$. Compare selected clones with control from all clones. Also see how persistent clones (appearing on all days) participate in the response.}

\begin{figure}[tbph!]
%\includegraphics{volcano}
%\centering{}
\caption{
\emph{Properties of significantly expanded clones}. The correlation between $f$ and $s_\textrm{median}$.
\label{fig:exp_prop}}
\end{figure}

\begin{figure}[tbph!]
\includegraphics{volcano}
\centering{}
\caption{
\emph{Hummingbird plot of confidence of response versus average effect size}. A significance threshold is placed according to $P_{\textrm{null}}=0.025$, where $P_{\textrm{null}}=P(s\leq 0)$ for expansion and $P_{\textrm{null}}=P(s\geq 0)$ for contraction. Inset shows the same threshold hold in $(n,n^\prime)$-space.
\label{fig:volcano}}
\end{figure}

\begin{figure}[tbph!]
\includegraphics{table_list}
\centering{}
\caption{
\emph{Overlap in list of significantly expanded clones.} The optimal values of $\alpha$ and $\bar{s}$ for donor S2 and day-0 day-15 comparison for 3 replicates (square markers). The background heat map is the list overlap $\left|\ell_{(\bar{s},\alpha)}\cap\ell^{\textrm{ref}}_{(\bar{s},\alpha)}\right|/\left|\ell_{(\bar{s},\alpha)}\cup\ell^{\textrm{ref}}_{(\bar{s},\alpha)}\right|$ with the reference given by the list obtained using values of $\bar{s}$ and $\alpha$ at the black dot.
%(a) $\rho(s)$,  \cref{eq:Ps_ex2}, is again parametrized by an effect size, $\bar{s}$, describing the expansion and contraction of the responding fraction, $\alpha$ of the repertoire. 
\label{fig:table_list}}
\end{figure}
% \subsection*{Ensemble-level application: Time-tracking of ensemble parameters}
% 
% \subsection*{Clone-level application: Identification of responding clones}
% 
% Here we infer the posteriors from the learned differential expression model and show their utility by using them to obtain a list of significantly expanded clones as a result of yellow fever vaccination. 
% \begin{figure}[ht!]
% %\includegraphics{fig2_null_model_example}
% %\centering{}
% \caption{
% \emph{Posteriors}. Some example posteriors. Distributions of slow, smed, shigh, and Pval. Volcano plot.
% \label{fig:posteriors}}
% \end{figure}
% 
% 
% \begin{itemize}
% 
% \item  discuss posteriors and expansion probability over observed repertoire ( \cref{fig:posteriors}): 
% \begin{itemize}
% 	\item nature of (0,n) posteriors a result of balance of rho(f) and rho(s) priors.
% 	\item The distribution of posterior expansion probability shifts for different priors but maintains the rank of expansion across clones. 
% \end{itemize}
% 
% \item  discuss significance tables.
% \begin{itemize}
% 	\item  How much does the structure of the prior change the order and size of significance tables: e.g. shift/other parameters of prior correlates with size of list, i.e. location of cutoff.
% 	\item  ...
% \end{itemize}
% 
% \item Is there any bias in the method?
% \begin{itemize}
% 	\item  Sample from model and do ROC-like analysis showing quality of discrimination in tables. E.g. tends to underestimate large fold change. 
% 	\item ... 
% \end{itemize} 
% 
% \end{itemize}


\section*{Discussion}
\re{this outline is out-dated. Didn't get chance to work on this yet.}
\begin{itemize}
	\item  Procedure Summary:
		\begin{itemize}
			\item  used replicates to determine experimental clone size variability
			\item  inferred repertoire change distributions 
			\item  used to determine significantly expanded clones
			\item  validated using functional assay
		\end{itemize}
	\item  Natural variation results and discussion:
		\begin{itemize}
			\item  universal same-day variation. Implications...
			\item  Data tightly constrains power law frequency. Implications...
		\end{itemize}
	\item  diffexpr results and discussion:
		\begin{itemize}
			\item  data strongly constrains prior expansion, not contraction or responding fraction. Implications...
			\item  Shift constraint and the relevance of homeostatis.

		\end{itemize}
    \item  application results and discussion:
		\begin{itemize}
			\item  posterior sensitivity to balance between $\alpha_f$ (prior for maximum expansion) and $\bar{s}$ (prior for characteristic expansion) in $(0,n)$ and $(n,0)$ pairs.
			\item  sensitivity of resulting tables. Note validation in Misha's paper.
		\end{itemize}
	\item  Clinical use (reference Misha paper)
	\item  drawbacks of approach: need replicate data, ...
\end{itemize}



\section*{Acknowledgements}
... would like to acknowledge discussions with ... MPT would like to thank M. Pogorelly for providing the R code used to obtain the EdgeR estimates in \citep{Pogorelyy12704}. This work was supported by ... 

\section*{Author Contributions}
M.P.T., A.W., T.M. ... 

\section*{Additional Information}
The authors declare no competing financial interests.
\section*{Appendices}
\appendix
%Our results are based on the posterior of differential fold change given an observed molecule count pair, which we infer directly from the data. Candidate responding clones were identified using a model of mRNA count statistics accounting for natural variation and differential expression and the sequencing process. Measured repertoires have a number of features that a model must capture to accurately reflect their statistics. First, for a clone to be observed it must appear in at least one of the pair of conditions, thus the support of the count distribution excludes $(n,n^{\prime})=(0,0)$. While containing $10^6$ clones, these measured repertoires are still less than 1 percent of the full repertoire.

% \subsection*{Null model definition}
% The normalized clone size, $f$, is distributed according to the probability density function $\rho(f)$, bounded by $f_{\textrm{min}}\leq f \leq 1$, where $f_{\textrm{min}}^{-1}$ is an estimate of the total number of lymphocytes in an individual. Based on previous observations (Cite Weinstein 2009, Mora 2010, Mora 2016), $\rho(f)$ is set as a power-law, i.e. $\rho(f)\propto f^{-\gamma}$. We considered four different statistical models of cells and mRNA molecules contained in a sample. These used a negative binomial distribution, with parametrized over-dispersion: $\mathrm{NegBin}(\mu, \sigma^2=\mu+a \mu^{\beta})$ and a Poisson distribution, $\mathrm{Poisson}(\bar{s})$.
% 
% The final and highest scoring model is of cells sampled from a negative binomial and mRNA molecules from a Poisson distribution. A clone of size $f$ appears in a sample containing $M$ lymphocytes on average as $fM$ cells. To account for over-dispersed count statistics, the number of cells is set to be Negative-Binomial distributed with mean $fM$ and variance $fM+a (fM)^{\gamma}$, with $a>0$ the coefficient and $\gamma>1$ the power controlling the over-dispersion. For each clone, the number $n$ of detected mRNA molecules (i.e. UMI) is distributed according to a Poisson distribution with mean $mN_{\textrm{reads}}/M$, where $N_{\textrm{reads}}/M$ is the average number of UMI per cell, obtained using the observed total number of molecules, $N_{\textrm{reads}}$.
%  
% We inferred the parameters of this null model, $\theta_n=(\alpha,M,a,\gamma)$, from day-0 replicates by maximizing the likelihood of the observed count pairs, $(n,n^{\prime})$, where $n\sim \mathrm{Poisson}(m N/M)$, $m\sim \mathrm{NegBin}(fM,fM+a (fM)^{\gamma})$, for each replicate, and $f\sim\rho$ is common to both replicates. For a given pair, the likelihood, $P(n,n^{\prime}|\theta_{\textrm{null}})$, is obtained by marginalizing over $m,m'$, and $f$.

\section{Normalization}\label{sec:normal}
Here we derive the condition for which the normalization in the joint density is implicitly satisfied. The normalization constant of the joint density is
\begin{equation}
	\mathcal{Z}=\int_{f_\textrm{min}}^1\cdots\int_{f_\textrm{min}}^1\prod_{i=1}^N \rho(f_i)\delta(Z-1)\textrm{d}^N\vec{f} \;\label{eq:joint},
\end{equation}
with $\delta(Z-1)$ being the only factor preventing factorization and explicit normalization. Writing the delta function in its Fourier representation factorizes the single constraint on $\vec{f}$ into $N$ Lagrange multipliers, one for each $f_i$,
\begin{align}
	\delta(Z-1)&=\int_{-i\infty}^{i\infty} \frac{\textrm{d} \mu}{2 \pi}e^{\mu(Z-1)}  \\
	&=\int_{-i\infty}^{i\infty} \frac{\textrm{d} \mu}{2 \pi}e^{-\mu}\prod_{i=1}^N e^{\mu f_i} \;.
\end{align}
Crucially, the multi-clone integral in \cref{eq:joint} over $\vec{f}$ then factorizes. Exchanging the order of the integrations and omitting the clone subscript without loss of generality,
\begin{equation}
	\mathcal{Z}=\int_{-i\infty}^{i\infty} \frac{\textrm{d} \mu}{2 \pi} e^{-\mu} \prod_{i=1}^N \langle e^{\mu f}\rangle\;,\label{eq:bigZ}
\end{equation}
with $\langle e^{\mu f}\rangle=\int_{f_\textrm{min}}^1\rho(f)e^{\mu f}\textrm{d}f$. Now define the large deviation function, $I(\mu):=-\frac{\mu}{N}+\log \langle e^{\mu f}\rangle$, so that 
\begin{equation}
	\mathcal{Z}=\int_{-i\infty}^{i\infty} \frac{\textrm{d} \mu}{2 \pi} e^{-N I(\mu)}\;.\label{eq:largedev}
\end{equation}
Note that $I(0)=0$. With $N$ large, this integral is well-approximated by the integrand's value at its saddle point, located at $\mu^*$ satisfying $I'(\mu^*)=0$.  Evaluating the latter gives
\begin{align}
	\frac{1}{N}&=\frac{\langle f e^{\mu^* f}\rangle}{\langle e^{\mu^* f}\rangle}\;.
\end{align} 
If the left-hand side is equal to $\langle f\rangle$, the equality holds only for $\mu^*=0$ since expectations of products of correlated random variables are not generally products of their expectations. 
In this case, we see from \cref{eq:largedev} that $\mathcal{Z}=1$, and so the constraint $N\langle f\rangle=1$ imposes normalization.
% \begin{align}
% 	\langle g(\vec{f})\rangle_{\rho_N(\vec{f})}&=\int_{f_\textrm{min}}^1\cdots\int_{f_\textrm{min}}^1\prod_{i=1}^N \rho(f)\frac{1}{2 \pi} \int_{-i\infty}^{i\infty} e^{i \mu(Z-1)} \textrm{d} \mu \textrm{d}^N\vec{f} g(\vec{f})\\
% 	\int_{f_\textrm{min}}^1\cdots\int_{f_\textrm{min}}^1\prod_{i=1}^N \rho(f)\frac{1}{2 \pi} \int_{-i\infty}^{i\infty} e^{i \mu(Z-1)} \textrm{d} \mu \textrm{d}^N\vec{f} g(\vec{f})\;.
% \end{align}

\section{Null model sampling}\label{sec:null_sampling}
The procedure for null model sampling is summarized as (1) fix main model parameters, (2) solve for remaining parameters using the normalization constraint, $N \langle f \rangle=1$, and (3) starting with frequencies, sample and use to specify the distribution of the next random variable in the chain.

In detail, we first fix:
\begin{itemize}
\item  the model parameters $(\alpha,M,a,\gamma)$, excluding $f_{\textrm{min}}$. Separate $M$, $a$, and $\gamma$ values could be defined for the reference and test condition, respectively. The empirical $P(n,n^{\prime})$ for replicate data was found to be highly symmetric in $n$ and $n^{\prime}$ across donors, however, supporting the assumption of a single acquisition model and so we neglect this complication. 
\item  the desired size of the full repertoire, $N$. 
%Together with the model prediction of capture efficiency, $1-P(0,0)$, this gives the number of clones we should sample, $N_{\textrm{\textrm{obs}}}$, via $N_{\textrm{\textrm{obs}}}=N(1-P(0,0))$, 
\item the sequencing efficiency (total sample reads/total sample cells), $\epsilon$. From this we get the effective total sample reads, $N^{\textrm{eff}}_{\textrm{reads}}=\epsilon M$, that converts a clone's frequency to the average number of cells it appears with in the sample. (We could in fact define two sequencing efficiencies, one for each replicate, leading to different effective total number of reads in each replicate). Note that the actual sampled number of reads is stochastic and so will differ from this fixed value.
\end{itemize}
We then solve for remaining parameters. Specifically, $f_{\textrm{min}}$ is fixed by the constraint that the average sum of all frequencies, under the assumption that their distribution factorizes, is unity:
\begin{equation}
% 	P(0,0)N\langle f\rangle_{\rho(f|n+n^{\prime}=0)} + N_{\textrm{\textrm{obs}}} \langle f\rangle_{\rho(f|n+n^{\prime}>0)} = 1
	N \langle f\rangle_{\rho(f)}=1
\end{equation}
This completes the parameter specification.

We then sample from the corresponding chain of random variables.
Sampling the chain of random variables of the null model can be performed efficiently by only sampling the $N_{\textrm{obs}}=N(1-P(0,0))$ observed clones. This is done separately for each replicate, once conditioned on whether or not the other count is zero. 
Samples with 0 molecule counts can in principle be produced with any number of cells, so cell counts must be marginalized when implementing this constraint. We thus used the conditional probability distributions $P(n|f)=\sum_{m}P(n|m)P(m|f)$ with $m,n=0,1,\dots$. $P(n^\prime|f)$ is defined similarly. Note that these two conditional distributions differ only in their average number of UMI per cell, $N_{\textrm{reads}}/M$, due to their differing number of observed total number of molecules, $N_{\textrm{reads}}$. Together with $\rho(f)$, these distributions form the full joint distribution, which is conditioned on the clone appearing in the sample, i.e. $n+n^{\prime}>0$ (denoted $\mathcal{O}$), 
\begin{align}
	P(n,n^{\prime},f|\mathcal{O})= \frac{P(n|f)P(n^{\prime}|f)\rho(f)}{1-\int{\textrm{d}f \rho(f)\textrm{d}f P(n=0|f)P(n^{\prime}=0|f)}}\;,  
\end{align}
with the renormalization accounting for the fact that $(n,n^{\prime})=(0,0)$ is excluded. The 3 quadrants having a finite count for at least one replicate are denoted $q_{x0}$, $q_{0x}$, and $q_{xx}$, respectively. Their respective weights are
\begin{align}
	P(q_{x0}|\mathcal{O})&=&\sum_{n>0}\int{\textrm{d}f P(n,n^{\prime}=0,f|\mathcal{O})}\;,\\
	P(q_{0x}|\mathcal{O})&=&\sum_{n^{\prime}>0}\int{\textrm{d}f P(n=0,n^{\prime},f|\mathcal{O})}\;,\\
	P(q_{xx}|\mathcal{O})&=&\sum_{\substack{n>0,\\n^{\prime}>0}}\int{\textrm{d}f P(n,n^{\prime},f|\mathcal{O})}.
\end{align}
Conditioning on $\mathcal{O}$ ensures normalization, $P(q_{x0}|\mathcal{O})+P(q_{0x}|\mathcal{O})+P(q_{xx}|\mathcal{O})=1$. Each sampled clone falls in one the three regions according to these probabilities. Their clone frequencies are then drawn conditioned on the respective region, 
\begin{align}
	P(f|q_{x0})&=&\sum_{n>0}P(n,n^{\prime}=0,f|\mathcal{O})/P(q_{x0}|\mathcal{O})\;,\\
	P(f|q_{0x})&=&\sum_{n^{\prime}>0}P(n=0,n^{\prime},f|\mathcal{O})/P(q_{0x}|\mathcal{O})\;,\\
	P(f|q_{xx})&=&\sum_{\substack{n>0,\\n^{\prime}>0}}P(n,n^{\prime},f|\mathcal{O})/P(q_{xx}|\mathcal{O}).
\end{align}

Using the sampled frequency, a pair of molecule counts for the three quadrants are then sampled as $(n,0)$, $(0,n^{\prime})$, and $(n,n^{\prime})$, respectively, with $n$ and $n^{\prime}$ drawn from the renormalized, finite-count domain of the conditional distributions, $P(n|f,n>0)$. 

% Using the sampled frequency, a pair of number of cells $(m,m^{\prime})$ is obtained. For $q_{x0}$, $m$ is sampled from $P(m|f,n>0)$ and $m^{\prime}$ sampled from $P(m^{\prime}|f,n^{\prime}=0)$ with 
% \begin{align}
% 	P(m|f,n>0)&=&\frac{\sum_{n>0}P(m,n|f)}{\sum_{\substack{n>0,\\m}}P(m,n|f)}\;,\\
% 	P(m^{\prime}|f,n^{\prime}=0)&=&\frac{P(m^{\prime},n^{\prime}=0|f)}{\sum_{m^{\prime}}P(m^{\prime},n^{\prime}=0|f)}\;,
% \end{align}
% with $P(m_i,n_i|f)=P(n_i|m_i)P(m_i|f)$, for $i=1,2$. Note that by construction here, $m>0$, since $P(n>0|m=0)=0$.
% The procedure is similar for frequencies sampled in $q_{0x}$. For frequencies sampled in $q_{xx}$, cell count pairs $(m,m^{\prime})$ are sampled from $P(m|f,n>0)$ and $P(m^{\prime}|f,n^{\prime}>0)$, respectively. 
% 
% Molecule counts for the three quadrants are then sampled as $(n,0)$, $(0,n^{\prime})$, and $(n,n^{\prime})$, respectively, with $n$ and $n^{\prime}$ drawn from the renormalized, finite-count domain of the conditional distributions, $P(n|m)$ and $P(n^{\prime}|m^{\prime})$, respectively, with $m>0$ and $m^{\prime}>0$. 

Using this sampling procedure we demonstrate the validity of the null model and its inference by sampling across the observed range of parameters and reinferring their values (See \cref{fig:SM_reinfer_null}).


\section{Differential model sampling}\label{sec:diffexpr_sampling}
Since the differential expression model involves expansion and contraction in the test condition, some normalization in this condition is needed such that it produces roughly the same total number of cells as those in the reference condition, consistent with the observed data. One approach (the one taken below) is to normalize at the level of clone frequencies. 
Here, we instead perform the inefficient but more straightforward procedure of sampling all $N$ clones and discarding those clones for which $(n,n^{\prime})=(0,0)$. A slight difference in the two procedures is that $N_{\textrm{obs}}$ is fixed in the former, while is stochastic in the latter.


\subsection*{Direct sampling}
The frequencies of the first condition, $f_i$, are sampled from $\rho(f)$ until they sum to 1 (i.e. until before they surpass 1, with a final frequency added that takes the sum exactly to 1). An equal number of log-frequency fold-changes, $s_i$, are sampled from $\rho(s)$. The normalized frequencies of the second condition are then $f'_i=f_ie^{s_i}/\sum_j f_je^{s_j}$.  Counts from the two conditions are then sampled from $P(n|f)$ and $P(n^{\prime}|f')$, respectively. Unobserved clones, i.e. those with $(n,n^{\prime})=(0,0)$, are then discarded.

% \subsubsection*{Effective sampling}
% For an efficient implementation, the procedure should avoid sampling the numerous clones that produce $(n,n^{\prime})=(0,0)$, since these are discarded. Such a procedure follows. 
% 
% First, the $(f,s)$-plane is partitioned into two regions, $D=\{(f,s)|f<f_0,fe^s<f_0\}$ and its complement, $\bar D$, with $f_0$ chosen such that clones sampled from $\bar D$ are often \textit{observed}, i.e. $n+n^{\prime}>0$ (a minority of clones sampled in $\bar D$ will nevertheless give $n+n^{\prime}=0$; these are discarded). In contrast, frequencies sampled from $D$ will be small, so that most will be unobserved, and we must condition on the clone being observed when sampling from this regime. Moreover, their average is unaffected by the long-tailed behaviour of the distribution in the large-frequency regime and thus is well-approximated by the corresponding ensemble average. We use this latter fact when computing the renormalization of the frequencies of the second condition. 
% 
% We compute the mass in $D$ as $P_D=\int_f \textrm{d}f\rho(f)\sum_s \rho(s) \mathbb{1}((f,s)\in D)$.
% 
% We sample $(f,s)$ in $\bar D$ until the sum of the first condition^{\prime}s frequencies, $\sum_i f_i$ added to the expected sum in $D$, $P_D N_{cl}\langle f\rangle_{P(f|D)}$, equals 1,
% \begin{align}
% 	1=\sum_{i=1}^{N_{\bar D}} f_i + P_D N_{cl}\langle f\rangle_{P(f|D)}\;,
% \end{align}
% where $N_{cl}$ is the total number of clones in the repertoire. The number sampled from $\bar D$, $N_{\bar D}$, is determined from that expression self-consistently by substituting $N_{cl}=N_{\bar D}/(1-P_D)$ obtained from $N_{\bar D}+P_D N_{cl}\equiv N_{cl}$. The normalization for the second condition^{\prime}s frequencies is then 
% \begin{align}
% 	Z=\sum_{i=1}^{N_{\bar D}} f_ie^{s_i} + P_D N_{cl}\langle fe^s\rangle_{P(f,s|D)}
% \end{align}
% such that the second condition^{\prime}s frequencies are ${f_i'=f_ie^{s_i}/Z}$. Molecule counts are then sampled from $P(n|f)$ and $P(n^{\prime}|f')$.  
% 
% We then sample from $D$ conditioned on the clone being observed, i.e. having produced a finite number of molecules in either of the two conditions. We thus sample $N_D=P(n+n^{\prime}>0|D)P_D N_{cl}$ clones from $P(f,s|D,n+n^{\prime}>0)$. To avoid having to sample over the joint distribution of $n$ and $n^{\prime}$, we condition on the 3 regions of finite counts in both conditions, $(n,0)$, $(0,n^{\prime})$, and $(n,n^{\prime})$, in which $n$ and $n^{\prime}$ can be sampled independently. 
% Note the presence of the normalization factor, $Z$, in 
% \begin{align}
% 	P(n+n^{\prime}>0|D)=\int_f \textrm{d}f\rho(f)\sum_s \rho(s)(1-P(n=0|f)P(n^{\prime}=0|f'=fe^s/Z))\;.
% \end{align}
% and
% \begin{align}
% 	P(f,s|D,n+n^{\prime}>0)=\frac{\rho(f)\rho(s)(1-P(n=0|f)P(n^{\prime}=0|f'=fe^s/Z))}{P(n+n^{\prime}>0|D)P_D}\;.
% \end{align}
% We then concatenate the $N_D$ sampled counts from $D$ and the $N_{\bar D}$ sampled counts (with $(n,n^{\prime})=(0,0)$ realizations discarded) from $\bar D$ to obtain the full data set.  

% \subsection*{Null Model Inference}
% Given a data set, $\mathcal{D}=\{(n_i,n^{\prime}_i)\}_{i=1}^{N_{\textrm{obs}}}$, we infer the parameters of the null model, $\Theta_{\textrm{null}}=(\alpha,a,\gamma,M,f_{\textrm{min}})$ by maximizing the marginal likelihood of the observable model, $P(n,n^{\prime}|n+n^{\prime}>0,\Theta_{\textrm{null}})$, subject to the normalization constraint$, N\langle f\rangle_{\rho(f)}=1$, where $N=N_{\textrm{obs}}/(1-P(0,0))$. Since in this case we have access to a realization, we could instead normalize conditioned on this realization. Since, 
% \begin{equation*}
% 	N\sum_{(n,n^{\prime})>0}P(n,n^{\prime})\approx N\sum_{(n,n^{\prime})\in \mathcal{D}}\frac{\#(n,n^{\prime})}{N}\equiv \sum_{i}^{N_{\textrm{obs}}}
% \end{equation*}
% we then have
% \begin{equation}
% 	1=N\langle f\rangle_{\rho(f|\mathcal{D})}
% 							   = P(0,0)N\langle f\rangle_{\rho(f|n+n^{\prime}=0)} + \sum_{i}^{N_{\textrm{obs}}}\langle f\rangle_{\rho(f|n_i,n^{\prime}_i)}\;.
% \end{equation}

% \subsection*{Differential expression model definition}
% We introduce a selection factor $s$ defined as the log-frequency fold-change between a clone's frequency on one day, $f$, and that on another, $fe^s$, and define $P(n,n^{\prime}|s,\theta_n)$ as before, but replacing $f$ by $fe^s$ in the definition of $m^{\prime}$. Given a prior distribution  $P(s|\theta_s)$ over $s$ parametrized by a set of parameters $\theta_s$ distinct from $\theta_n$, we used Bayes rule to obtain the posterior log-frequency fold-change probability function given an observed count pair, $P(s|n,n^{\prime},\theta_n,\theta_s)$. We set the values of the parameters $\theta_s$ of this prior by again maximizing the likelihood of the count pair data given the model over $\theta_s$, $\int P(n,n^{\prime}|s,\theta_n)P(s|\theta_s)\textrm{d}s$.
% 
% We explored a family of priors expressible as 
% \begin{eqnarray}
% 	P(s|\theta_s)&=&\frac{\alpha \beta}{Z_+}e^{-\frac{|s-s_0|}{s_+}}\Theta(s-s_0) \nonumber\\
% 	&&+\frac{\alpha (1-\beta)}{Z_-}e^{-\frac{|s-s_0|}{s_-}}\Theta(s_0-s) \\
% 	&&+(1-\alpha)\delta(s-s_0)\;, \label{eq:genPs}\nonumber
% \end{eqnarray}
% with $Z_{\pm} \sim s_{\pm}$ (see  \cref{fig:Ps}) so that in the most general prior, $\theta_s=(s_-,s_+,\alpha,\beta,s_0)$. See table \ref{Tbl1:Priors} for reduced-parameter versions of this model that we considered.


\section{Obtaining diversity estimates from the clone frequency density}\label{sec:infer_div}
For a set of clone frequencies, $\{f_i\}_{i=1}^{N}$, for a set of clones, the Hill family of diversities are obtained from the Renyi entropies, as $D_\beta=\exp H_\beta$, with $H_\beta=\frac{1}{1-\beta}\ln \left[ \sum_{i=1}^N f_i^{\beta}\right]$. We use $\rho(f)$ to compute their ensemble averages over $f$, again under the assumption that the joint distribution of frequencies factorizes. We obtain an estimate for $D_0=N$ using the model-derived expression, $N_{\textrm{samp}}+P(n=0)N=N$, where $N_{\textrm{samp}}$ is the number of clones observed in one sample, and $P(n=0)=\int_{f_{\textrm{min}}}^1 P(n=0|f)\rho(f)\textrm{d}f$. For $\beta=1$, we compute $\exp (N\langle -f\log f \rangle_{\rho(f)})$ and for $\beta=2$, we use $1/\left(N\langle f^2\rangle_{\rho(f)}\right)$.

\section{Deriving update for shift, $s_0$} \label{sec:shift_proc}
The constraint of equal repertoire size, $Z^\mathcal{D}_{f'}=Z^\mathcal{D}_f$ (\cref{eq:fprimeconst}), can be satisfied with a suitable choice of the shift parameter, $s_0$, in the prior for differential expression, $\rho_{s_0}(s)$, namely $s_0=-\ln Z^\mathcal{D}_{f'}/Z^\mathcal{D}_{f}$. The latter arises from the coordinate transformation $s\leftarrow\Delta s+s_0$ that maps $\rho_{s_0}(s)$ to $\rho_{0}(\Delta s)$, and adds a factor of $e^{s_0}$ to all terms of $Z^\mathcal{D}_{f'}$.
% \begin{align}
% 	P(f,s|n,n^{\prime})&=&\frac{P(n,n^{\prime},f,s)}{\int\textrm{d}f\int\textrm{d}s P(n,n^{\prime},f,s)}
% \end{align}
% and using this, $P(f|n+n^{\prime}>0)=\int\textrm{d}s P(f,s|n,n^{\prime})$. 
% 
% \re{need to mention that the realization specific constraints are used here.}
% 
% The shift enters in $P(n,n^{\prime},f,s)=P(n|f)P(n^{\prime}|f,s)\rho(f)\rho_{s_0}(s)$ via $\rho_{s_0}(s)$. A convenient change of variables $s\leftarrow\Delta s+s_0$ maps $\rho_{s_0}(s)$ to $\rho_{0}(\Delta s)$, upon which
% \begin{align}
% 	\langle fe^s \rangle &=&\int{ \textrm{d}f \sum_{\Delta s} fe^{\Delta s+s_0} P(n|f)P(n^{\prime}|f,\Delta s+s_0)\rho(f)\rho_{0}(\Delta s)} \\
% 						 &=&e^{s_0}\int{ \textrm{d}f \sum_{\Delta s} fe^{\Delta s} P(n|f)P(n^{\prime}|f,\Delta s+s_0)\rho(f)\rho_{0}(\Delta s)}\;.
% \end{align}
% Denoting the remaining integral, $\tilde{\langle fe^s \rangle}$, and performing the same change of variables on $\langle f \rangle$,
% \begin{align}
% 	\langle f \rangle &=&\int{ \textrm{d}f \sum_{\Delta s} f             P(n|f)P(n^{\prime}|f,\Delta s+s_0)\rho(f)\rho_{0}(\Delta s)}\;,
% \end{align}
% the condition can be written as $s_0=\ln \tilde{\langle fe^s \rangle} - \ln \langle f \rangle$. To obtain $s_0$ from this implicit equation, we apply an iterative scheme beginning with $s_0=0$. We compute $P(n^{\prime}|f,\Delta s+s_0)$, and then the latter expression supplies $s_0$ in the next iteration. In practice, we take a bounded range of $\Delta s$ symmetric around 0. Thus, the only factor containing shift information is $P(n^{\prime}|f,\Delta s+s_0)$ appearing in both $\tilde{\langle fe^s \rangle}$ and $\langle f \rangle$. However, for self-consistent numerics, the $e^{\Delta s}$ factor must be defined over a shifted domain.

\section{Prior solvable via expectation maximization}\label{sec:EM}
For learning the parameters of $\rho(s)$, we performed a grid search, refined by an iterative, gradient-based search to obtain the maximum likelihood. In a more formal approach, here we employ expectation maximization (EM) to obtain the optimal parameter estimates from the data by calculating the expected log likelihood over the posterior and then maximizing with respect to the parameters. In practise, we first perform the latter analytically and then evaluate the former numerically. We choose a symmetric exponential as a tractable prior for this purpose:
\begin{align}
	\rho_{\bar{s}}(s)=e^{-|s|/\bar{s}}/2\bar{s}
\end{align}
with $s\in\mathbb{R}$, $\bar{s}>0$, and no shift. The expected value of the log likelihood function, often called the Q-function in EM literature, is 
 \begin{align}
 Q(\bar{s}|\bar{s}')=\sum_{i=1}^{N_\textrm{\textrm{obs}}}\int_{-\infty}^{\infty}\mathrm{d}s\rho(s|n_i,n_i^{\prime},\bar{s}')\log \left[P(n_i,n_i^{\prime},s|\bar{s})\right]\;,
 \end{align}
 where $\bar{s}'$ is the current estimate.
 Maximizing $Q$  with respect to $\bar{s}$ is relatively simple since $\bar{s}$ appears only in $\rho_{\bar{s}}(s)$  which is a factor in $P(n,n^{\prime},s|\bar{s})$. For each $s$,
 \begin{align}
 \frac{\partial \log \left[\rho_{\bar{s}}(s))\right] }{\partial\bar{s}} &=\frac{1}{\rho_{\bar{s}}(s)} \frac{\partial\rho_{\bar{s}}(s)}{\partial\bar{s}}\\&=\frac{|s|-\bar{s}}{\bar{s}^2}\;,
 \end{align}
so that $  \frac{\partial Q(\bar{s}|\bar{s}')}{\partial\bar{s}}=\sum_{i=1}^{N_\textrm{obs}}\int_{-\infty}^{\infty}\mathrm{d}s\rho(s|n_i,n_i^{\prime},\bar{s}')\frac{\partial \log \left[\rho_{\bar{s}}(s))\right] }{\partial\bar{s}} =0$ implies
\begin{align}
  \sum_{i=1}^{N_\textrm{obs}}\int_{-\infty}^{\infty}\mathrm{d}s\rho(s|n_i,n_i^{\prime},\bar{s}')\frac{|s|-\bar{s}^*}{\bar{s}^{*2}} =0
\end{align}
so that $\bar{s}^*=\frac{1}{N_\textrm{\textrm{obs}}}\sum_{i=1}^{N_\textrm{obs}}\bar{s}_{(n_i,n_i^{\prime})}$, where 
\begin{align}
\bar{s}_{(n,n^{\prime})}=\int_{-\infty}^{\infty}\mathrm{d}s|s|\rho(s|n,n^{\prime},\bar{s}').
\end{align}
The latter integral is computed numerically from the model using $\rho(s|n,n^{\prime},\bar{s}')=P(n,n^{\prime},s|\bar{s}')/\int_{-\infty}^{\infty}P(n,n^{\prime},s|\bar{s}')\mathrm{d}s	$. $Q$ is maximized at $\bar{s}=\bar{s}^*$ since  $ \frac{\partial^2 \log \left[\rho_{\bar{s}}(s))\right] }{\partial\bar{s}^2}\bigg|_{\bar{s}=\bar{s}^*}=-\bar{s}^{*-2} <0$. Thus, we update $\rho_{\bar{s}}(s)$ with 
\begin{align}
\rho_{\bar{s}}(s)\leftarrow\rho_{\bar{s}^*}(s).
\end{align}
The number of updates typically required for convergence was small.


\section{Identifying responding clones}
In analogy with $p$-values, we used the posterior probability corresponding to the null hypothesis that they are not expanded, $p=\rho(s\leq s_{thr}|n,n^{\prime},\theta_n,\theta_s)$ to rank the clones by the significance of their expansion, using a threshold of $p<0.025$ and a threshold effect size of $s_{thr}$. 



%\lange fe^s \rangle &=&\int{ \textrm{d}f \sum_s fe^s p(f|n+n^{\prime}>0)}\;,

% Candidate responding clones were identified using a model of RNA count statistics accounting for differential expression and the sequencing process. A full presentation will be published elsewhere. Here, we provide a brief exposition of the model and how we have applied it. The core repertoire object in the model is a probability density function, $\rho(f)$, of normalized clone size, called clone frequency, $f\in[1/m_{total},1]$, where $m_{total}=10^{11}$ is an estimate of the total number of lymphocytes in an individual. $\rho(f)\mathrm{d}f$ is then the probability that a randomly sampled clone takes up a fraction $f$ of the repertoire. We set $\rho(f)\propto f^{\gamma}$, i.e. power-law distributed with the power $\gamma<0$, consistent with many observed unpartitioned sampled repertoires.
% 
% Next, a blood sample contains a sampled repertoire in the form of an a priori unknown number of lymphocytes, $m_{sample}$. Across many clones of size $f$ in the sample, these will appear on average with $\bar{m}(f)=fm_{sample}$ cells. In our data and elsewhere, prevalent overdispersion is observed in the count statistics. We thus employ the mean-variance relation, $\sigma_m^2(f)=\bar{m}(f)+a \bar{m}(f)^{\beta}$, with $a>0$ the coefficient and $\beta>1$ the power controlling the over-dispersion. We set the number of cells to be Negative-Binomial distributed with mean $\bar{m}(f)$ and variance $\sigma_m^2$. 
% 
% Finally, the cells in the sample are then barcode-sequenced, resulting in a number of putative RNA molecules, $n$, for each clone. $m$ cells will produce $\bar{n}(m)=mr_c$ molecules on average where $r_c>0$ is the average number of RNA molecules per cell. $r_c$ brings together into a single parameter a variety of diluting effects, heterogeneous across samples so that we expect $r_c<1$. For an total observed number of observed reads, $n_{sample}$, $r_c$ is related to $m_{sample}$ via $n_{sample}=r_cm_{sample}$. Even though $m_{sample}$ is precisely controlled by the sampled volume, $n_{sample}$ still varies due to procedural variation in sequencing. Thus, we let $r_c$ account for this variation by setting it as $r_c=m_{sample}/n_{sample}$. We set the number of molecules to be Poisson-distributed with scale parameter $\bar{n}(f)$.
% 
% We used day-0 replicates to infer maximum likelihood estimates for these parameters using the likelihood function,
% \begin{align}
% \mathcal{L}(\gamma,m_{sample},a,\beta)=\frac{1}{N_{clones}}\sum_{(n,n^{\prime})_{data}}\log P_{\mathrm{same}}(n,n^{\prime})
% \end{align}
% where $N_{clones}$ is the number of observed clones and
% \begin{align}
% P_{\mathrm{same}}(n,n^{\prime})&=\int_{1/m_{total}}^1P(n|f=f)P(n^{\prime}|f^{\prime}=f)\rho(f)\mathrm{d}f\\
% P(n|f)&=\sum_{m=0}^{\infty}\mathcal{NB}_{\bar{m}(f),\sigma^2_{m}(f)}(m)Pois_{\bar{n}(m)}(n)\;.
% \end{align}
% This fitted model characterizes the variation expected of pairs of sampled repertoires for unchanged antigen conditions.
% 
% For changing antigen conditions, the clone frequencies making up the repertoire will change. Using $s$ to denote the log-frequency fold-change between a clone's frequency in one condition, $f$, and another $f^{\prime}=fe^s$, a distribution of these changes, $\rho(s)$, exists. A particular $\rho(s)$ serves as a prior distribution for these changes. The posterior probability given an observed count pair is then
% \begin{align}
% P(s|n,n^{\prime})\propto P(n,n^{\prime}|s)\rho(s)
% \end{align}
% where 
% \begin{align}
% P(n,n^{\prime}|s)=\int_{1/m_{total}}^1P(n|f=f)P(n^{\prime}|f^{\prime}=fe^s)\rho(f)\mathrm{d}f\;.
% \end{align}
% Note the additional factor of $e^s$ compared with the same-day model in the expression for $f^{\prime}$. 
% The posterior distribution of fold change given an observed count pair can be used to rank the clones by the significance of their expansion. In analogy with p-values, we use the posterior probability corresponding to the null hypothesis that they are not expanded, $P(s\leq 0|n,n^{\prime})$ and set a threshold of significance $P(s\leq 0|n,n^{\prime})\leq 0.025$. 
% 
% We parametrize our prior, $\rho(s)$, with  $\alpha\in[0,1]$, the fraction of clones that responds to the change, and $\bar{s}>0$, their typical effect size. Accordingly, we set $\rho(s)=\frac{\alpha}{Z}\exp\left[-|s|/\bar{s}\right]+(1-\alpha)\delta(s)$, where the normalization, $Z$, is such that the first term integrates to $\alpha$. We set the values of the parameters of this prior by again maximizing the likelihood of the data given the corresponding model,
% \begin{align}
% \mathcal{L}(\alpha,\bar{s})=\frac{1}{N_{clones}}\sum_{(n,n^{\prime})_{data}}\log P_{\mathrm{diff}}(n,n^{\prime})
% \end{align}
% where
% \begin{align}
% P_{\mathrm{diff}}(n,n^{\prime})=\int_{s_{min}}^{s_{max}}P(n,n^{\prime}|s)\rho(s)\mathrm{d}s\;.
% \end{align}



	
%An asymmetric exponential decay away from a shifted center at which a non-responding fraction is placed (see  \cref{eq:genPs} and  \cref{fig:Ps}).

% \begin{figure}[h!]
% \includegraphics{schematic}
% \centering{}
% \caption{
% \emph{Functional forms of prior on clone size log-frequency fold-change}. $\rho(s)$,  \cref{eq:genPs}, is parametrized most generally here by an effect size for both expansion, $s_+$, and contraction,$s_-$, with an expanded fraction, $\beta$ of changed clones, the latter fraction of which itself a parameter, $\alpha$. Expansion and contraction is relative to the functions center, $s_0$, which can be fixed by the constraint $\langle f \rangle=\langle f^{\prime} \rangle$.
% \label{fig:Ps}}
% \end{figure}


%\begin{table}%[H] add [H] placement to break table across pages
%\caption{Prior functions used. See  \cref{fig:Ps} and  \cref{eq:genPs}. $\langle f \rangle= \langle f^{\prime} \rangle$ adds constraint, e.g. fixes $s_0$. \label{Tbl1:Priors}}
%\begin{ruledtabular}
%\begin{tabular}{c|c|c|c|c|c|c|c}
%index	& label				&$\alpha$ & $\beta$	& $s_-$    	& $s_+$		& $s_0$  & \#DOF ($s_0$ fixed) \\
%\hline
%0 		& $flat$ shift 	 	&$\alpha$ & $1/2$  	& $\infty$	& $\infty$  & $s_0$  & 2 \\
%1 		& $L=R$ 	 		&$\alpha$ & $1/2$  	& $\bar{s}$	& $\bar{s}$ & $0$    & 2 \\
%2 		& $L=R$ shift		&$\alpha$ & $1/2$  	& $\bar{s}$	& $\bar{s}$ & $s_0$  & 3(-1) \\
%3 		& $R$ only   		&$\alpha$ & $0$    	& n/a       & $\bar{s}$ & $0$    & 2 \\
%4 		& $R$ only shift  	&$\alpha$ & $0$  	& n/a       & $\bar{s}$ & $s_0$  & 3(-1) \\
%5 		& $L\neq R$  		&$\alpha$ & $\beta$	& $s_-$    	& $s_+$     & $0$    & 4 \\
%6 		& $L\neq R$ shift  	&$\alpha$ & $\beta$	& $s_-$    	& $s_+$     & $s_0$  & 5(-1)
%\end{tabular}
%\end{ruledtabular}
%\end{table}

\begin{figure}[ht!]
%\includegraphics{procedure.png}
%centering{}
\caption{
\emph{Supp. Fig: \re{Not Done!}Approximate equivalence of $N\langle f \rangle=1$ and $\mathcal{Z}^{\mathcal{D}}_f$.\label{fig:SM_match_null_constr}}}
\end{figure}

\begin{figure}[ht!]
\includegraphics[width=\linewidth]{figS2}
\centering{}
\caption{
\emph{Supp. Fig: Two-step model captures tail better than one-step model.\label{fig:SM_twostep_better}}}
\end{figure}

\begin{figure}[ht!]
\includegraphics[width=\linewidth]{NB_Pois_nullpara_fits}
\centering{}
\caption{
\emph{Reinferring null model parameters}. Shown are the actual and estimated values of the null model parameters used to validate the null model inference procedure over the range exhibited by the data. A 3x3x3x3 grid of points were sampled and results collapsed over each parameter axis. $f_{min}$ was fixed to satisfy the normalization constraint.
\label{fig:SM_reinfer_null}}
\end{figure}

\begin{figure}[ht!]
\includegraphics[width=\columnwidth]{synthetic_reinference}
\centering{}
\caption{
\emph{Supp. Fig: Precise, self-consistent reinference of differential expression model for human-sized repertoire}.
\label{fig:SM_reinf_diffexpr}}
\end{figure}

\begin{figure}[ht!]
\includegraphics[width=\columnwidth]{s_naive_hist}
\centering{}
\caption{
\emph{Supp. Fig: Empirical histograms of naive log-frequency fold-change for day-0/day-0 and day-0/day-15 pair comparisons}.
\label{fig:SM_snaive_hists}}
\end{figure}

\begin{figure}[ht!]
\includegraphics{s_med_vs_snaive}
\centering{}
\caption{
\emph{Supp. Fig: Summary statistics of log-frequency fold-change posterior distributions}. (a) summary statistics. (b) comparing the posterior median log-frequency fold-change and the naive estimate, $\log n^{\prime}/n$ (across clones with $n,n^{\prime}>0$).
\label{fig:SM_smed_snaive}}
\end{figure}

% \begin{figure}[ht!]
% %\includegraphics{fig2_null_model_example}
% %\centering{}
% \caption{
% \emph{Evolution of parameters}. 
% \label{fig:timeevo}}
% \end{figure}

% \begin{comment}
% \begin{itemize}
%   \item Supp fig: show self-consistent inference (See  \cref{fig:suppfig2_reinf_diffexpr}).
%   \item Explore space of fold-change priors (see Table \ref{Tbl1:Priors}) for representative 0-15 comparison (now just Azh, soon to be corroborated by results from all donors).
%   \begin{itemize}
%     \item  Prior 0: $\mathcal{L}^*=-4.2$, for $s^*_0\approx-3$, $\alpha^*=0$. At $s^*_0$, $\langle f^{\prime}\rangle/\langle f\rangle\approx10^{-3}.$
% 	\item  Prior 1: $\mathcal{L}^*=-1.952$, for $\bar{s}^*\to \infty $, $\alpha^*\to 0$: high plateau to left of edge ridge for $\alpha \propto \bar{s}^{-4}$
% 	\item  Prior 2: $\mathcal{L}^*=-1.96$. Same ridge, but with negative slope: $\bar{s}^*\to 0 $, $\alpha^*\to 1$.
% 	\item  Prior 3: Same as 2 for small $s_0$ and $\alpha$. slightly lower for larger values (-0.3). I.e. negative lobe has no effect for no shift.
% 	\item  Prior 4: Coming soon!
% 	\item  Prior 5: $\mathcal{L}^*=-1.953$, for $\bar{s}_+^*=1.0$, $\alpha^*=0.07$, $\bar{s}_-^*=0.86$, $\beta^*=0.11$ . small $\alpha$ is penalized (not so in prior 3,4).  %Similar to prior 2 except $\beta<1$ means that can get expanded right, independent of alpha, so that then too small alpha can be penalized. 
% 	\item  Prior 6: $\mathcal{L}^*=-1.954$, for $\bar{s}^*=0.67$, $\alpha^*=0.45$, $\bar{s}_-^*=0.9$, $\beta^*=0.11$. 
% 	\item  ...	still need to do free shift for Prior 1-6.
%   \end{itemize}
% 
%   \item Using $\langle f\rangle=\langle f^{\prime}\rangle$ constraint to set $s_0$, gives $s^*_0<0$, which pulls down the likelihood at small $\alpha$, sculpting an interior maximum.
% 
%   \item Dynamics of ensemble parameters (See  \cref{fig:timeevo}): 
%   \begin{itemize}
% 	\item  the learned parameters, e.g. effect size and response fraction, give low response for same day (0-0, 0-pre0) comparisons.
% 	\item  Across days they grow and decay with the response. 
% 	\item  pre0 and 7 are similar to 0 (all compared to 0)?
% 	\item  pre0 is as different from 0 as day 7. 
%   \end{itemize} 
% 
%   \item  Depending on legitimacy of pre0-0 null model, how does the prior parameter dynamics differ? are forward/backward distribution time-reversible?
% \end{itemize}
% \end{comment}
% (old) Text:
% 
% Armed with a well fit model of the same day variation, we generalized the model to apply to different days using a log-frequency fold-change variable sampled from some distribution that controls the expansion or contraction that the second clone exhibits. 
% And, marginalizing out the hidden fold change variable, we again obtain a joint count distribution that we can use to fit the parameters of rho(s) to maximize the likelihood of the data. 
% We choose to restrict rho(s) to a two-parameter family of symmetric functions, with a parameter alpha the fraction of the repertoire that responds, and the sbar a typical effect size that controls the mean of a symmetric exponential function around 0 log-frequency fold-change where the rest of clones are left as a delta function component.
% Doing the inference on alpha and sbar, and using the day 0 as the reference, we saw how the fraction of the perturbed repertoire rises and falls around day 15, as expected. 




% Old Text from presentation:
% 
% To do that we look at the posterior probability of a clone being expanded or contracted for a given count pair, where here we plot it as a confidence and versus the pair’s expected fold change that comes out the model, a volcano plot, commonly used in RNAs
% We set a threshold on confidence and all pairs above this threshold are candidates for being selective to the vaccine. 
% Take a look at the two sets of points associated with the first count being 0 and 1, and with the second count of the data points increases with the expected fold change. 
% Remember that low counts convey little frequency and thus expansion information: the initial frequency could have been anything small and so we have to go to pretty high fold change, i.e. high n2, before we can say that this expansion is significant. 
% In particular, if we condition instead on the first count being 1, the expansion becomes significant at lower fold change, as the first count more reliably conveys information about it’s frequency. and so on with increasing count, and similarly for contraction.
% These are candidate clones ‘specific to the vaccines’. But how do we do we know that this is in fact the case.
% Experimental work performed a functional test by adding a gate for IFN gamma and sequenced this IFN enriched pool indeed show the vast majority show a strong functional response, validating our method. 


\bibliography{diffexpr}

\end{document}  












