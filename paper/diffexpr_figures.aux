\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibdata{diffexpr_figuresNotes}
\bibstyle{apsrev4-1}
\citation{REVTEX41Control}
\citation{apsrev41Control}
\newlabel{FirstPage}{{}{1}{}{section*.1}{}}
\@writefile{toc}{\contentsline {title}{Inferring repertoire dynamics and using them to identify responsive clones}{1}{section*.2}}
\@writefile{toc}{\contentsline {abstract}{Abstract}{1}{section*.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  \emph  {Learning a null model}. \textbf  {A} Clone frequency distribution, $\rho (f)$, is set as a power law, parameterized by the power, $\alpha $ and the minimum frequency, $f_{min}$. \textbf  {B} Null model parameters are learned by marginalizing over clone frequency, $f$, and maximizing this marginal likelihood with respect to the parameters. Sampled repertoires can then be generated with the ML estimate. \textbf  {C} Models and data comparison using molecule pair count statistics (example donor: S2; day 0-day 0 comparison). The learned model for Poisson distributed $P(n|f)$ (left) and for $P(n|f)$ set as a negative binomial distribution in cell counts controlling the scale parameter of a Poisson distribution of molecule counts (center; fig.\ref  {fig:modelstruct}). Right: empirical pair count histogram. Gray scale bar denotes the empirical frequency of a $(n,n')$ pair. }}{1}{figure.1}}
\newlabel{fig:nullstats}{{1}{1}{\emph {Learning a null model}. \textbf {A} Clone frequency distribution, $\rho (f)$, is set as a power law, parameterized by the power, $\alpha $ and the minimum frequency, $f_{min}$. \textbf {B} Null model parameters are learned by marginalizing over clone frequency, $f$, and maximizing this marginal likelihood with respect to the parameters. Sampled repertoires can then be generated with the ML estimate. \textbf {C} Models and data comparison using molecule pair count statistics (example donor: S2; day 0-day 0 comparison). The learned model for Poisson distributed $P(n|f)$ (left) and for $P(n|f)$ set as a negative binomial distribution in cell counts controlling the scale parameter of a Poisson distribution of molecule counts (center; fig.\ref {fig:modelstruct}). Right: empirical pair count histogram. Gray scale bar denotes the empirical frequency of a $(n,n')$ pair}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  \emph  {Differential expression model structure and learning procedure}. $P(n|f)$ is set as a negative binomial distribution in cell counts controlling the scale parameter of a Poisson distribution of molecule counts. In the differentially expressed condition (prime-decorated quantities), the parameters, $\theta _{diff}$, of the prior distribution, $\rho (c)$, of fold-change, $c$, are learned by maximizing the marginal likelihood (see section \textit  {Fold change prior}) with respect to $\theta _{diff}$, keeping the remaining parameters, $\theta _{null}$, fixed to their ML estimates, $\mathaccentV {hat}05E{\theta }_{null}$, previously obtained using same-day replicate data (see fig. \ref  {fig:nullstats}). }}{2}{figure.2}}
\newlabel{fig:modelstruct}{{2}{2}{\emph {Differential expression model structure and learning procedure}. $P(n|f)$ is set as a negative binomial distribution in cell counts controlling the scale parameter of a Poisson distribution of molecule counts. In the differentially expressed condition (prime-decorated quantities), the parameters, $\theta _{diff}$, of the prior distribution, $\rho (c)$, of fold-change, $c$, are learned by maximizing the marginal likelihood (see section \textit {Fold change prior}) with respect to $\theta _{diff}$, keeping the remaining parameters, $\theta _{null}$, fixed to their ML estimates, $\hat {\theta }_{null}$, previously obtained using same-day replicate data (see fig. \ref {fig:nullstats})}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  \emph  {Learned null model parameters}. Shown is the same data from fig. \ref  {fig:nullparas}, plotted separately for each donor and time point. Error bars are the inverse standard deviation of a Gaussian approximation around the maximum of the likelihood, i.e. the Cramer-Rao lower bound of the variance of the estimator. }}{3}{figure.3}}
\newlabel{fig:nullparas_timeseries}{{3}{3}{\emph {Learned null model parameters}. Shown is the same data from fig. \ref {fig:nullparas}, plotted separately for each donor and time point. Error bars are the inverse standard deviation of a Gaussian approximation around the maximum of the likelihood, i.e. the Cramer-Rao lower bound of the variance of the estimator}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  \emph  {Diversity estimates.} Shown are diversity estimates obtained from the Renyi entropies, $H_\beta $, of the inferred clone frequency distributions for $\beta =1$ (Shannon entropy) and $\beta =2$ (Simpson index), across donors and days. }}{3}{figure.4}}
\newlabel{fig:div_estimates}{{4}{3}{\emph {Diversity estimates.} Shown are diversity estimates obtained from the Renyi entropies, $H_\beta $, of the inferred clone frequency distributions for $\beta =1$ (Shannon entropy) and $\beta =2$ (Simpson index), across donors and days}{figure.4}{}}
\newlabel{LastPage}{{}{3}{}{}{}}
